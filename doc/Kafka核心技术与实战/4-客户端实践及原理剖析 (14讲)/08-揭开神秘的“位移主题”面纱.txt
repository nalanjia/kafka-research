你好，我是胡夕。今天我要和你分享的内容是：Kafka 中神秘的内部主题（Internal Topic）__consumer_offsets。

__consumer_offsets 在 Kafka 源码中有个更为正式的名字，叫位移主题，即 Offsets Topic。为了方便今天的讨论，我将统一使用位移主题来指代 __consumer_offsets。需要注意的是，它有两个下划线哦。

好了，我们开始今天的内容吧。首先，我们有必要探究一下位移主题被引入的背景及原因，即位移主题的前世今生。

在上一期中，我说过老版本 Consumer 的位移管理是依托于 Apache ZooKeeper 的，它会自动或手动地将位移数据提交到 ZooKeeper 中保存。当 Consumer 重启后，它能自动从 ZooKeeper 中读取位移数据，从而在上次消费截止的地方继续消费。这种设计使得 Kafka Broker 不需要保存位移数据，减少了 Broker 端需要持有的状态空间，因而有利于实现高伸缩性。

但是，ZooKeeper 其实并不适用于这种高频的写操作，因此，Kafka 社区自 0.8.2.x 版本开始，就在酝酿修改这种设计，并最终在新版本 Consumer 中正式推出了全新的位移管理机制，自然也包括这个新的位移主题。

新版本 Consumer 的位移管理机制其实也很简单，就是将 Consumer 的位移数据作为一条条普通的 Kafka 消息，提交到 __consumer_offsets 中。可以这么说，__consumer_offsets 的主要作用是保存 Kafka 消费者的位移信息。它要求这个提交过程不仅要实现高持久性，还要支持高频的写操作。显然，Kafka 的主题设计天然就满足这两个条件，因此，使用 Kafka 主题来保存位移这件事情，实际上就是一个水到渠成的想法了。

这里我想再次强调一下，和你创建的其他主题一样，位移主题就是普通的 Kafka 主题。你可以手动地创建它、修改它，甚至是删除它。只不过，它同时也是一个内部主题，大部分情况下，你其实并不需要“搭理”它，也不用花心思去管理它，把它丢给 Kafka 就完事了。

虽说位移主题是一个普通的 Kafka 主题，但它的消息格式却是 Kafka 自己定义的，用户不能修改，也就是说你不能随意地向这个主题写消息，因为一旦你写入的消息不满足 Kafka 规定的格式，那么 Kafka 内部无法成功解析，就会造成 Broker 的崩溃。事实上，Kafka Consumer 有 API 帮你提交位移，也就是向位移主题写消息。你千万不要自己写个 Producer 随意向该主题发送消息。

你可能会好奇，这个主题存的到底是什么格式的消息呢？所谓的消息格式，你可以简单地理解为是一个 KV 对。Key 和 Value 分别表示消息的键值和消息体，在 Kafka 中它们就是字节数组而已。想象一下，如果让你来设计这个主题，你觉得消息格式应该长什么样子呢？我先不说社区的设计方案，我们自己先来设计一下。

首先从 Key 说起。一个 Kafka 集群中的 Consumer 数量会有很多，既然这个主题保存的是 Consumer 的位移数据，那么消息格式中必须要有字段来标识这个位移数据是哪个 Consumer 的。这种数据放在哪个字段比较合适呢？显然放在 Key 中比较合适。

现在我们知道该主题消息的 Key 中应该保存标识 Consumer 的字段，那么，当前 Kafka 中什么字段能够标识 Consumer 呢？还记得之前我们说 Consumer Group 时提到的 Group ID 吗？没错，就是这个字段，它能够标识唯一的 Consumer Group。

说到这里，我再多说几句。除了 Consumer Group，Kafka 还支持独立 Consumer，也称 Standalone Consumer。它的运行机制与 Consumer Group 完全不同，但是位移管理的机制却是相同的。因此，即使是 Standalone Consumer，也有自己的 Group ID 来标识它自己，所以也适用于这套消息格式。

Okay，我们现在知道 Key 中保存了 Group ID，但是只保存 Group ID 就可以了吗？别忘了，Consumer 提交位移是在分区层面上进行的，即它提交的是某个或某些分区的位移，那么很显然，Key 中还应该保存 Consumer 要提交位移的分区。

好了，我们来总结一下我们的结论。位移主题的 Key 中应该保存 3 部分内容：。如果你认同这样的结论，那么恭喜你，社区就是这么设计的！

接下来，我们再来看看消息体的设计。也许你会觉得消息体应该很简单，保存一个位移值就可以了。实际上，社区的方案要复杂得多，比如消息体还保存了位移提交的一些其他元数据，诸如时间戳和用户自定义的数据等。保存这些元数据是为了帮助 Kafka 执行各种各样后续的操作，比如删除过期位移消息等。但总体来说，我们还是可以简单地认为消息体就是保存了位移值。

当然了，位移主题的消息格式可不是只有这一种。事实上，它有 3 种消息格式。除了刚刚我们说的这种格式，还有 2 种格式：

1。用于保存 Consumer Group 信息的消息。
2。用于删除 Group 过期位移甚至是删除 Group 的消息。

第 1 种格式非常神秘，以至于你几乎无法在搜索引擎中搜到它的身影。不过，你只需要记住它是用来注册 Consumer Group 的就可以了。

第 2 种格式相对更加有名一些。它有个专属的名字：tombstone 消息，即墓碑消息，也称 delete mark。下次你在 Google 或百度中见到这些词，不用感到惊讶，它们指的是一个东西。这些消息只出现在源码中而不暴露给你。它的主要特点是它的消息体是 null，即空消息体。

那么，何时会写入这类消息呢？一旦某个 Consumer Group 下的所有 Consumer 实例都停止了，而且它们的位移数据都已被删除时，Kafka 会向位移主题的对应分区写入 tombstone 消息，表明要彻底删除这个 Group 的信息。

好了，消息格式就说这么多，下面我们来说说位移主题是怎么被创建的。通常来说，当 Kafka 集群中的第一个 Consumer 程序启动时，Kafka 会自动创建位移主题。我们说过，位移主题就是普通的 Kafka 主题，那么它自然也有对应的分区数。但如果是 Kafka 自动创建的，分区数是怎么设置的呢？这就要看 Broker 端参数 offsets.topic.num.partitions 的取值了。它的默认值是 50，因此 Kafka 会自动创建一个 50 分区的位移主题。如果你曾经惊讶于 Kafka 日志路径下冒出很多 __consumer_offsets-xxx 这样的目录，那么现在应该明白了吧，这就是 Kafka 自动帮你创建的位移主题啊。

你可能会问，除了分区数，副本数或备份因子是怎么控制的呢？答案也很简单，这就是 Broker 端另一个参数 offsets.topic.replication.factor 要做的事情了。它的默认值是 3。

总结一下，如果位移主题是 Kafka 自动创建的，那么该主题的分区数是 50，副本数是 3。

当然，你也可以选择手动创建位移主题，具体方法就是，在 Kafka 集群尚未启动任何 Consumer 之前，使用 Kafka API 创建它。手动创建的好处在于，你可以创建满足你实际场景需要的位移主题。比如很多人说 50 个分区对我来讲太多了，我不想要这么多分区，那么你可以自己创建它，不用理会 offsets.topic.num.partitions 的值。

不过我给你的建议是，还是让 Kafka 自动创建比较好。目前 Kafka 源码中有一些地方硬编码了 50 分区数，因此如果你自行创建了一个不同于默认分区数的位移主题，可能会碰到各种各种奇怪的问题。这是社区的一个 bug，目前代码已经修复了，但依然在审核中。

创建位移主题当然是为了用的，那么什么地方会用到位移主题呢？我们前面一直在说 Kafka Consumer 提交位移时会写入该主题，那 Consumer 是怎么提交位移的呢？目前 Kafka Consumer 提交位移的方式有两种：自动提交位移和手动提交位移。

Consumer 端有个参数叫 enable.auto.commit，如果值是 true，则 Consumer 在后台默默地为你定期提交位移，提交间隔由一个专属的参数 auto.commit.interval.ms 来控制。自动提交位移有一个显著的优点，就是省事，你不用操心位移提交的事情，就能保证消息消费不会丢失。但这一点同时也是缺点。因为它太省事了，以至于丧失了很大的灵活性和可控性，你完全没法把控 Consumer 端的位移管理。

事实上，很多与 Kafka 集成的大数据框架都是禁用自动提交位移的，如 Spark、Flink 等。这就引出了另一种位移提交方式：手动提交位移，即设置 enable.auto.commit = false。一旦设置了 false，作为 Consumer 应用开发的你就要承担起位移提交的责任。Kafka Consumer API 为你提供了位移提交的方法，如 consumer.commitSync 等。当调用这些方法时，Kafka 会向位移主题写入相应的消息。

如果你选择的是自动提交位移，那么就可能存在一个问题：只要 Consumer 一直启动着，它就会无限期地向位移主题写入消息。

我们来举个极端一点的例子。假设 Consumer 当前消费到了某个主题的最新一条消息，位移是 100，之后该主题没有任何新消息产生，故 Consumer 无消息可消费了，所以位移永远保持在 100。由于是自动提交位移，位移主题中会不停地写入位移 =100 的消息。显然 Kafka 只需要保留这类消息中的最新一条就可以了，之前的消息都是可以删除的。这就要求 Kafka 必须要有针对位移主题消息特点的消息删除策略，否则这种消息会越来越多，最终撑爆整个磁盘。

Kafka 是怎么删除位移主题中的过期消息的呢？答案就是 Compaction。国内很多文献都将其翻译成压缩，我个人是有一点保留意见的。在英语中，压缩的专有术语是 Compression，它的原理和 Compaction 很不相同，我更倾向于翻译成压实，或干脆采用 JVM 垃圾回收中的术语：整理。

不管怎么翻译，Kafka 使用 Compact 策略来删除位移主题中的过期消息，避免该主题无限期膨胀。那么应该如何定义 Compact 策略中的过期呢？对于同一个 Key 的两条消息 M1 和 M2，如果 M1 的发送时间早于 M2，那么 M1 就是过期消息。Compact 的过程就是扫描日志的所有消息，剔除那些过期的消息，然后把剩下的消息整理在一起。我在这里贴一张来自官网的图片，来说明 Compact 过程。
【08-配图-使用Compact策略删除过期消息.jpeg】

图中位移为 0、2 和 3 的消息的 Key 都是 K1。Compact 之后，分区只需要保存位移为 3 的消息，因为它是最新发送的。

Kafka 提供了专门的后台线程定期地巡检待 Compact 的主题，看看是否存在满足条件的可删除数据。这个后台线程叫 Log Cleaner。很多实际生产环境中都出现过位移主题无限膨胀占用过多磁盘空间的问题，如果你的环境中也有这个问题，我建议你去检查一下 Log Cleaner 线程的状态，通常都是这个线程挂掉了导致的。

小结

总结一下，今天我跟你分享了 Kafka 神秘的位移主题 __consumer_offsets，包括引入它的契机与原因、它的作用、消息格式、写入的时机以及管理策略等，这对我们了解 Kafka 特别是 Kafka Consumer 的位移管理是大有帮助的。实际上，将很多元数据以消息的方式存入 Kafka 内部主题的做法越来越流行。除了 Consumer 位移管理，Kafka 事务也是利用了这个方法，当然那是另外的一个内部主题了。

社区的想法很简单：既然 Kafka 天然实现了高持久性和高吞吐量，那么任何有这两个需求的子服务自然也就不必求助于外部系统，用 Kafka 自己实现就好了。
【08-配图-Kafka的位移主题__consumer_offsets.jpg】

开放讨论

今天我们说了位移主题的很多好处，请思考一下，与 ZooKeeper 方案相比，它可能的劣势是什么？

欢迎写下你的思考和答案，我们一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。

精选留言(54)


海贼王
相对于zookeeper方案可能的劣势是，kafka得保证offset topic的读写都是线性一致的。

但是有个疑惑，如果kafka自己实现了类似zab协议的话，那写性能会比zk高吗
作者回复: hmm.... 哈哈哈，不知道。。。。
2019-07-09

2

7

mellow
老师能讲一下，同一个group下的consumer启动之后是怎么去offset topic 拿到该group上次消费topic每个partition的最新offset呢？是根据key来定位offset topic的partition吗，然后拿到所有消息得到最新的offset吗
作者回复: 它会去寻找其Coordinator Leader副本对应的broker去拿。根据group.id找到对应Coordinator的分区数
2019-07-09


5

蛋炒番茄
“自动提交位移有一个显著的优点，就是省事，你不用操心位移提交的事情，就能保证消息消费不会丢失”，对于这一点我表示疑问啊❓我记得你在之前的章节里面讲过自动提交不仅会增加消息可重复消费的可能，也可能导致部分消息丢失。比如说虽然消息拉取下来但是还没消费完就已经提交，此时服务挂了这样情况。
2019-07-09


4

耿斌
与 ZooKeeper 方案相比，它可能的劣势；
想到的是，当集群中有一台 Broker 故障下线，可能会造成 __consumer_offset 丢失，导致重复消费
2019-07-24

2

3

Coder4
老师好，前几年一直有个说法，说kafka不适合创建过多topic，请问现在的新版还有这个问题么？
作者回复: topic过多其实是指分区数过多。会有两个可能的问题：1. controller无法管理这么多分区；2. 分区数过多导致broker物理随机IO增加，减少吞吐量。

第一个问题社区算是修复了吧，目前单controller能够支持20w的分区数，况且社区也在考虑做多controller方案；第二个问题目前没有太多直接的修复举措，只能说具体问题具体分析吧
2019-07-11


2

永光
位移主题，适用于高频写的操作，为什么ZooKeeper不适用于这种高频的写操作？zookeeper 也可以按照<Group ID，主题名，分区号 > 来写入呀？
作者回复: ZooKeeper本身只是一个分布式协调框架，znode中保存的数据多是那些不怎么频繁修改的元数据，本身不适合频繁更新。

是的，旧版本consumer就是这么使用ZooKeeper来保存位移的
2019-07-11

1

2

nightmare
比如多个线程同时消费一个分区的话，位移这么处理
2019-07-09

3

2

yyyiue
请问offset是以最新的为准，还是值最大的为准？
作者回复: 最新的
2019-07-09

1

2

注定非凡
1，诞生背景
A ：老版本的Kafka会把位移信息保存在Zk中，当Consumer重启后，自动从Zk中读取位移信息。这种设计使Kafka Broker不需要保存位移数据，可减少Broker端需要持有的状态空间，有利于实现高伸缩性。
B ：但zk不适用于高频的写操作，这令zk集群性能严重下降，在新版本中将消费者的位移数据作为一条条普通的Kafka消息，提交至内部主题（_consumer_offsets）中保存。实现高持久性和高频写操作。

2，特点:
A ：位移主题是一个普通主题，同样可以被手动创建，修改，删除。。
B ：位移主题的消息格式是kafka定义的，不可以被手动修改，若修改格式不正确，kafka将会崩溃。
C ：位移主题保存了三部分内容：Group ID，主题名，分区号。

3，创建：
A ：当Kafka集群中的第一个Consumer程序启动时，Kafka会自动创建位移主题。也可以手动创建
B ：分区数依赖于Broker端的offsets.topic.num.partitions的取值，默认为50
C ：副本数依赖于Broker端的offsets.topic.replication.factor的取值，默认为3

4，使用：
A ：当Kafka提交位移消息时会使用这个主题
B ：位移提交得分方式有两种:手动和自动提交位移。
C ：推荐使用手动提交位移，自动提交位移会存在问题：只有consumer一直启动设置，他就会无限期地向主题写入消息。

5，清理：
A ：Kafka使用Compact策略来删除位移主题中的过期消息，避免位移主题无限膨胀。
B ：kafka提供专门的后台线程定期巡检待compcat的主题，查看是否存在满足条件的可删除数据。

6，注意事项：
A ：建议不要修改默认分区数，在kafka中有些许功能写死的是50个分区
B ：建议不要使用自动提交模式，采用手动提交，避免消费者无限制的写入消息。
C ：后台定期巡检线程叫Log Cleaner，若线上遇到位移主题无限膨胀占用过多磁盘，应该检查此线程的工作状态。
2019-11-03


1

🤡
对GroupId 还有疑惑，假设一个Group下有 3 个Consumer , 那这三个Consumer 对应的groupid 应该是一样的。这样的话怎么做key做唯一区分呢
作者回复: 每个client都有自己的member id和client id用于区分彼此
2019-08-12


1

宋晓明
老师 消息从producer到broker里的partition其实都是有序的，这是kafka的机制保证的，那么假如我的consumer是单线程的，也能保证消费是有序的，但是吞吐量就下降了。如果consumer是多线程，如果保证有序性？
作者回复: 如果是多个分区，即使是但consumer线程，也没法保证全局的顺序性。这是无法规避的
2019-07-30


1

ban
老师，你说偏移量都保存到了__consumer_offsets，确实看到生成了，但是为什么我在zk还是能看到偏移量信息，记录在
consumers/{group}/offsets/{topic}/{partition}。
这两个有什么区别吗，还是说zk会记录还是因为我们的程序手动改成提交到zk里面了。
作者回复: 如果使用使用老版本consumer，还是会记录在ZooKeeper中的
2019-07-10

1

1

玉剑冰锋
不好意思老师，地铁上有点匆忙提问的问题没有描述清楚，我想的问的是1.文章提到__consumer_offset这个topic记录的offset信息和zk中记录的offset信息有啥区别？
2.__consumer_offset这个topic我线上环境没有副本，文章提到副本默认是3，是我配置的问题吗？发现这个问题以后我就在线添加副本，过去好几天了仍然有十几个in progress，另外有没有办法强制终止掉这个任务重新添加副本？
作者回复: 1. 没什么区别。你可以简单认为就是换个地方保存：）
2. 这个其实是个bug，不过现在已经修复了。如果reassign hang住了，手动删除Zk下对应的znode就能恢复
2019-07-10


1

玉剑冰锋
现在kafka中同样还在使用zk,每个主题也会记录位移，跟主题位移有啥区别？另外我线上主题位置没有副本，之前从来没动过，这是啥原因导致的？
作者回复: “每个主题也会记录位移，跟主题位移有啥区别” 这是什么意思呢？没看懂。。。。

“我线上主题位置没有副本” 这又是什么意思呢。。。
2019-07-10


1

nico
大神 Kafka可以发送 定时消息吗，制定某一时刻接受到消息 拜托🙏
作者回复: 需要自己写代码实现，Kafka没有天然提供这个功能
2019-07-09

1

1

张天屹
对于自动提交位移这里有两点疑惑，第一个老师说“能够避免消息丢失”，那如果自动提交之后，业务处理失败呢，不久丢失消息了吗？第二个老师说“最新消息为100，没有继续生产，这个时候消费者会不断自动提交最新位移100”，既然没有消费了，为什么还要提交呢？消费了100就提交100，之后没有消费就意味着位移没变，为啥还要提交呢？这两个问题的根源都在于不是很清楚自动位移提交的触发条件，是消费就触发吗？还是没有发生异常就触发？还是定时触发？
作者回复: 现在自动提交位移的设计就是不管你有没有消费，就是阶段性地提交位移，即使是提交相同的位移
2019-11-28



feifei
为什么我的kafka consumer自动创建的位移主题没有副本，就只有50个分区
作者回复: 比较老的版本中offsets.topic.replication.factor参数值并不会被严格遵守，你可以手动将__consumer_offsets的rf值调高
2019-11-18



pain
那么消费者怎么获取到自己要消费的那个分区的位移呢？是通过消费位移主题的数据吗？
作者回复: 对的，要消费的位移是通过从位移主题获取的
2019-10-29



寂静欢喜
想问老师，consumer offset 本身就是个主题。它是怎么实现自己offset 管理的
作者回复: Kafka自己管理的，其实和普通主题原理是类似的
2019-10-21



王藝明
老师好！
为什么位移主题写入消息时，不直接替换掉原来的数据，像 HashMap 一样呢？而是要堆积起来，另起线程来维护位移主题
作者回复: 位移主题也是主题，也要遵循Kafka底层的日志设计思路，即append-only log
2019-10-14

1


Mick
老师,请问下为什么我的位移主题的key(<Group ID，主题名，分区号 >) 没有分区号 ，只有Group ID和主题名。
作者回复: 我记得应该只有groupid吧，甚至连主题名都没有。位移主题里面有两类消息：一类是保存消费者消费位移的消息，key是<groupId, topic, partition>；另一类是组注册消息，key是groupid
2019-09-27



大牛凯
老师好，请教一个小白问题，一个位移主题是在第一个consumer启动时建立的，是说对于一个kafka集群只有一个位移主题么？另外我对kafka框架还是有点迷惑，kafka集群是不是没有NameNode这个概念啊？每一个leader partition就相当于一个NameNode？谢谢老师
作者回复: 每个Kafka集群都只有一个位移主题。Kafka没有NameNode的概念。如果硬搬Hadoop中的概念，我倒倾向于认为分区的leader副本是datanode，而namenode的作用在Kafka中由ZooKeeper承接
2019-09-23



小可
老师，单机版kafka首次启动的时候，第一个消费者连接kafka，但是由于一些原因比如其他程序当时在占用IO写磁盘，导致无法创建位移主题，重启消费者也不会再创建位移主题，重启kafka才可以。我看了kafka源码，在KafkaApis的getOrCreateInternalTopic方法中，它先从缓存metadataCache读取位移主题的，客户端第一次连接kafka时缓存中已经创建了的位移主题，但没有落盘；客户端重启再次连接时读到了缓存，也不会触发创建的位移主题的动作，也就无法落盘。不知道我的理解是否正确？像这种情况除了重启kafka还有没有其他处理办法？
作者回复: 可以手动创建位移主题
2019-09-19



DFighting
分区的位移信息需要保证数据一致性吧，万一丢失了怎么办？替换zk难到自己也实现zab分布式一致性协议？还是说有什么地方可以恢复位移丢失的数据？或者是支持at-least-once语义？关于写性能的提升，至少有一点吧，放在自己的topic里，写zk的网络时延应该会减少不少吧。
2019-09-05



giantbroom
其实无脑的把redis抓过来写位移主题信息不是挺好么，自动去重，自动持久化。另外，两个下划线，这位开发老兄是不是有很深的C/C++背景 :)
2019-09-02



开水
老师，想问一下0.9版本new consumer api真的将偏移量都存到位移主题了么？我实验后发现相同group.id，consumer重启之后会重复消费，只有在consumer还存活的时候，才能通过Kafka-consumer-group.sh中查询到偏移量，是不是这个版本的偏移量保存还存在bug呢？
作者回复: 0.9新版本consumer刚刚推出，一堆bug，真的不要用
2019-08-29



阿猫阿狗
与Zookeeper相比的劣势是：如果broker集群出现问题崩溃，可能消费的唯一信息也会丢失，再重启后不能接着之前的位移继续读取。但是如果存在外部的Zookeeper中，就不会有这个问题。
2019-08-22



godtrue
1：位移主题？
1-1：是kafka的一个自定义主题，本质和普通主题没什么区别
1-2：主要作用是为了保存kafka中各种consumer的位移信息
1-3：位移主题的消息格式有kafka定义，其中key的格式如下——<groupId,主题名,分区号>，消息体比较复杂，但可以简单认为它注意是保存了一个位移信息
1-4：位移主题，有三种格式，
1-4-1：用于管理consumer的位移
1-4-2：用于保存 consumer group 的信息
1-4-3：用于删除过期的group位移及消息
1-5：当 Kafka 集群中的第一个 Consumer 程序启动时，Kafka 会自动创建位移主题。
1-6：如果位移主题是 Kafka 自动创建的，那么该主题的分区数是 50，副本数是 3。建议不要手动创建
1-7：删除也有两种方式，自动删除和手动删除，自动删除不灵活，手动删除更可控
1-8：引入位移主题的原因是，zookeeper不合适频繁的写入，kafka的设计天然支持快速持久化和高吞出量
1-10：位移主题，通常不需要认为干预，让其自我管理就行了

如果不巧存储kafka位移主题信息的分区的broker都宕机了，那其他的主题消息是否也无法消费啦？

另外，手动写位移主题的信息，如果不小心将格式弄错了，会导致broker的崩溃，那broker是不是设计的有些脆弱？
2019-08-16



小鱼
老师，你好，请问控制Kafka 使用Compact 策略来删除位移主题中的过期消息的参数是哪个？

作者回复: offsets.retention.minutes
2019-07-31



King Yao
老师你好，最近我们有在扩容Kafka。请问在主题迁移的时候，位移主题是否需要迁移调整，谢谢
作者回复: 可以调整也可以不调整，主要是确保位移主题的副本数满足预期就好
2019-07-24

1


其实我很屌
位移主题默认副本是1，建议在正文中提示一下。这个参数很要命
2019-07-18



given
consumer端 日常业务发版呢，那每次发版需要重启consumer不是也会导致Rebalance，这个如何规避
作者回复: 可以考虑使用standalone consumer，否则group机制无法避免
2019-07-15



wykkx
老师我想请教几个问题，
1，具体是从哪个版本开始，位移数据开始默认的不存在zk而是存在自己内部了？
2，如果验证目前使用的环境，位移是存在内部还是zk上？
3，什么场景下适合使用自动提交位移？
作者回复: 1. 0.9
2. 推荐保存在Kafka内部
3. 不在乎重复消费
2019-07-13



子华
你好，请教下。发现线上offsets.topic.replication.factor=1 然后通过官网教程 https://kafka.apache.org/documentation/#basic_ops_increase_replication_factor 在线调整了位移主题的分区数3 ，也反馈成功了。但是还是不知道 这种调整方式 对于 __consumer_offsets 这种特殊主题是否有效。

因为发现 比如 __consumer_offsets-46 这个主题，之前leader的文件有
-rw-rw-r-- 1 www www 0 7月 10 14:59 00000000000000000000.index
-rw-rw-r-- 1 www www 2620 7月 10 14:59 00000000000000000000.log
-rw-rw-r-- 1 www www 12 7月 10 14:59 00000000000000000000.timeindex
-rw-rw-r-- 1 www www 10 6月 18 18:02 00000000000000000189.snapshot
-rw-rw-r-- 1 www www 10 6月 20 17:59 00000000000000000203.snapshot
-rw-rw-r-- 1 www www 10 7月 3 15:00 00000000000000000224.snapshot
-rw-rw-r-- 1 www www 10485760 7月 12 18:11 00000000000000065983.index
-rw-rw-r-- 1 www www 3091267 7月 12 18:11 00000000000000065983.log
-rw-rw-r-- 1 www www 10 7月 10 15:00 00000000000000065983.snapshot
-rw-rw-r-- 1 www www 10485756 7月 12 18:11 00000000000000065983.timeindex
-rw-rw-r-- 1 www www 28 7月 12 17:14 leader-epoch-checkpoint

但是同步后两个副本文件大概如下
-rw-rw-r-- 1 www www 10485760 7月 12 18:09 00000000000000000000.index
-rw-rw-r-- 1 www www 3093887 7月 12 18:11 00000000000000000000.log
-rw-rw-r-- 1 www www 10485756 7月 12 18:09 00000000000000000000.timeindex
-rw-rw-r-- 1 www www 23 7月 12 17:14 leader-epoch-checkpoint

很明显同步后的副本没有00000000000000065983.index这个文件，但是通过describle 命令看 都是Isr状态
Topic: __consumer_offsets	Partition: 46	Leader: 0	Replicas: 0,1,2	Isr: 0,2,1

不知道这种状态是否OK？
2019-07-12



酱排骨
老师，我想问一下工作中的一个问题单个消费实例，单个partion，消费者消费失败，offset可以重新回到前面位置重新消费吗？
作者回复: 消费者重启回来后会从最新一次提交的位移处继续消费
2019-07-11



巧克力黑
老师，你好
Kafka有位移主题，是不是Consumer都是从整个位移主题获取数据应该从哪个offset开始读数据，如果Spark Streaming作为一个Consumer，其offset的控制也是在这个位移主题当中？ 这样的话Spark Streaming用Direct方式读取Kafka其实是不需要额外找其他存储作为位移的保存？
作者回复: Spark Streaming集成Kafka的位移管理有三种：1. 寿司用checkpoint，也就是保存在Spark内部；2. 保存在Kafka内部主题；3. 保存在外部存储中。你可以选择合适的方式。
2019-07-11



MoonGod
请教老师一个问题：
在consumer提交位移的时候，通过Coordinator 往所在的broker写消息，那如果当前的broker挂掉了，写入位移主题的消息会丢失吗？还是说位移主题在写入的时候也会把消息同步到其他broker中的副本中，从而保证写入消息不丢失呢
作者回复: 不会。位移主题的failover和高可用管理和普通Kafka topic是一样的。也会执行leader选举。
2019-07-11



MoonGod
请教老师一个问题：
在consumer提交位移的时候，通过Coordinator 往所在的broker写消息，那如果写入的broker挂掉了，写入位移主题的消息会丢失吗？还是说位移主题在写入的时候也会把消息同步到其他broker中的副本中，从而保证写入消息不丢失呢
2019-07-11



无菇朋友
老师，offset topic是在coordinator对应的broker上创建且只创建一次是么？
作者回复: offset topic在整个集群上被创建出来，并且只会创建一次
2019-07-10



南辕北辙
老师，请教一下consumer 是如何从这个位移主题中拿到曾经属于自己组的offset呢
作者回复: 首先找到对应的Coordinator，Coordinator保存了这些数据，然后consumer向Coordinator发送请求去请求这些数据
2019-07-10



dream
老师，你最后说“Kafka 提供了专门的后台线程定期地巡检待 Compact 的主题”，那么这个定期是好久呢？这个间隔时间我们可以自己配置吗？
作者回复: 没有相关配置。我的“定期”的意思是有个专门的线程不断地去做这件事。。。。
2019-07-09



AF
劣势就是zk是基于内存的，而位移主题要保存到broker上，也就是磁盘……
2019-07-09



AF
老师能解释一下zk不适合频繁读写的原理吗？
2019-07-09



演技熊
个人想法，与 ZK 方案相比，写入到自身集群中使 Kafka 集群本身变成了一个有状态的服务，不利于服务本身的扩展吧，如果其中某个 broker 宕机，可能会对 offset 的提交有影响。

另外，手动提交 offset 在一些场景下比较有用。

比如有10条消息，
offset key 
1 A
2 B
3 C
4 A
5 B
6 C
7 A
8 A
9 A
10 B

如果我对同样 Key 的消费顺序有严格要求，那么我必须分成 3 组来分别顺序处理，
A 组
1 A
4 A
7 A
8 A
9 A

B组
2 B
5 B
10 B

C组
3 C
6 C

假如每个消息的处理时间都差不多的话，那么很有可能 B 组的 10 号比 A 要早处理完，那么这时如果自动提交 offset ，很可能会导致 A 组产生消息丢失风险。
除非 10 条消息完全串行处理，但是这是没有必要的，分成三组处理总时间取决于处理最久的那一组。


2019-07-09



WL
请教老师三个问题：
1. 在consumer group中的一个consumer消费一条消息后，是往它拉取消息的那个broker写一条offset消息还是往所有它连接的broker都广播一条消息。
2. 一个broker中的位移主题保存的是他自己上面的主题和分区的位移还是整个集群的所有主题所有分区的位移都有保存。
3. 位移主题的50个分区分配在各个broker的方式是啥，轮询，hash，还是随机？
作者回复: 1. 你是指提交位移吗？如果是，它是向对应的Coordinator所在的broker发送一条位移写请求
2. 都有保存
3. 你基本上可以认为是轮训的
2019-07-09

1


无菇朋友
胡老师，刚接触kafka，很多不了解的地方，有个问题：为什么offset需要由consumer端来提交，比如有这么一种情况：
消费者组c1去消费某个topic的分区0，从0开始消费，一次拉取了10条消息，那么consumer拉取成功后，回给broker一个ack，broker收到ack后，往__consumer_offsets主题里写入消息，比如c1-topic-0-10
2019-07-09

2


燃烧的M豆
老师如果超大规模集群超大规模消费者对这一个 50 个 partitions 的 topic 进行消费是不是会引起性能问题？topic 下的 partitions 设置有上线吗 面对超大规模并发除了提升 partitions 数量还有什么办法？谢谢
作者回复: partition数量在kafka中没有上限，只受os限制。除了提升partition，增加broker挂载的磁盘数也是一个方法
2019-07-09



陈华应
老师，consumer_offset可以理解为使用topic实现了一个k-v存储吗？consumer从里面消费消息的时候不是那种顺序消费，而是根据key来查找最新的value这种方式吗？compact的时候应该不会阻止consumer继续发送位移消息，那使用的是什么样的“垃圾回收”算法呢？
作者回复: 如果一定要对比JVM GC算法，那么它使用的就是sweep-compact算法
2019-07-09



lmtoo
我对这个compact的过程比较感兴趣，是拷贝原始日志最新的消息到一个新的日志文件，还是直接在原始日志上做删除操作？这个工作量应该非常大
2019-07-09



吃饭饭
__consumer_offsets 这个主题的元数据信息还是会在 Zookeeper 中注册吧？只是位移的信息变更存储在了 Kafka 中吗？
作者回复: 不是。位移提交这件事从设计到实现与ZooKeeper完全没有关联了。
2019-07-09



never leave
如果kafka消息没有key的话，怎么Compact？比较value吗？
作者回复: compact topic必须要求有key
2019-07-09



蒙开强
老师，你好，如果某条信息消费失败了，那我重新消费那可以获取刚刚失败信息的offset么，如果放zk，那是可以获得最大消费的offset的
2019-07-09



monkay
一旦某个 Consumer Group 下的所有 Consumer 实例都停止了，而且它们的位移数据都已被删除时，Kafka 会向位移主题的对应分区写入 tombstone 消息，表明要彻底删除这个 Group 的信息。


什么情况下会彻底删除GROUP的信息呢？如果GROUP下所有consumer实例都同时停掉重启会出现这种情况吗？
作者回复: 都已经停到然后位移数据已被删除
2019-07-09



曾轼麟
zookeeper是HA模式，如果使用topic就没法做到数据强一致性
2019-07-09

