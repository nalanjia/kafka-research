你好，我是胡夕。今天我要和你分享的主题是：Kafka 的消费者组。

消费者组，即 Consumer Group，应该算是 Kafka 比较有亮点的设计了。那么何谓 Consumer Group 呢？用一句话概括就是：Consumer Group 是 Kafka 提供的可扩展且具有容错性的消费者机制。既然是一个组，那么组内必然可以有多个消费者或消费者实例（Consumer Instance），它们共享一个公共的 ID，这个 ID 被称为 Group ID。组内的所有消费者协调在一起来消费订阅主题（Subscribed Topics）的所有分区（Partition）。当然，每个分区只能由同一个消费者组内的一个 Consumer 实例来消费。个人认为，理解 Consumer Group 记住下面这三个特性就好了。

1。Consumer Group 下可以有一个或多个 Consumer 实例。这里的实例可以是一个单独的进程，也可以是同一进程下的线程。在实际场景中，使用进程更为常见一些。
2。Group ID 是一个字符串，在一个 Kafka 集群中，它标识唯一的一个 Consumer Group。
3。Consumer Group 下所有实例订阅的主题的单个分区，只能分配给组内的某个 Consumer 实例消费。这个分区当然也可以被其他的 Group 消费。

你应该还记得我在专栏第 1 期中提到的两种消息引擎模型吧？它们分别是点对点模型和发布 / 订阅模型，前者也称为消费队列。当然，你要注意区分很多架构文章中涉及的消息队列与这里的消息队列。国内很多文章都习惯把消息中间件这类框架统称为消息队列，我在这里不评价这种提法是否准确，只是想提醒你注意这里所说的消息队列，特指经典的消息引擎模型。

好了，传统的消息引擎模型就是这两大类，它们各有优劣。我们来简单回顾一下。传统的消息队列模型的缺陷在于消息一旦被消费，就会从队列中被删除，而且只能被下游的一个 Consumer 消费。严格来说，这一点不算是缺陷，只能算是它的一个特性。但很显然，这种模型的伸缩性（scalability）很差，因为下游的多个 Consumer 都要“抢”这个共享消息队列的消息。发布 / 订阅模型倒是允许消息被多个 Consumer 消费，但它的问题也是伸缩性不高，因为每个订阅者都必须要订阅主题的所有分区。这种全量订阅的方式既不灵活，也会影响消息的真实投递效果。

如果有这么一种机制，既可以避开这两种模型的缺陷，又兼具它们的优点，那就太好了。幸运的是，Kafka 的 Consumer Group 就是这样的机制。当 Consumer Group 订阅了多个主题后，组内的每个实例不要求一定要订阅主题的所有分区，它只会消费部分分区中的消息。

Consumer Group 之间彼此独立，互不影响，它们能够订阅相同的一组主题而互不干涉。再加上 Broker 端的消息留存机制，Kafka 的 Consumer Group 完美地规避了上面提到的伸缩性差的问题。可以这么说，Kafka 仅仅使用 Consumer Group 这一种机制，却同时实现了传统消息引擎系统的两大模型：如果所有实例都属于同一个 Group，那么它实现的就是消息队列模型；如果所有实例分别属于不同的 Group，那么它实现的就是发布 / 订阅模型。

在了解了 Consumer Group 以及它的设计亮点之后，你可能会有这样的疑问：在实际使用场景中，我怎么知道一个 Group 下该有多少个 Consumer 实例呢？理想情况下，Consumer 实例的数量应该等于该 Group 订阅主题的分区总数。

举个简单的例子，假设一个 Consumer Group 订阅了 3 个主题，分别是 A、B、C，它们的分区数依次是 1、2、3，那么通常情况下，为该 Group 设置 6 个 Consumer 实例是比较理想的情形，因为它能最大限度地实现高伸缩性。

你可能会问，我能设置小于或大于 6 的实例吗？当然可以！如果你有 3 个实例，那么平均下来每个实例大约消费 2 个分区（6 / 3 = 2）；如果你设置了 8 个实例，那么很遗憾，有 2 个实例（8 – 6 = 2）将不会被分配任何分区，它们永远处于空闲状态。因此，在实际使用过程中一般不推荐设置大于总分区数的 Consumer 实例。设置多余的实例只会浪费资源，而没有任何好处。

好了，说完了 Consumer Group 的设计特性，我们来讨论一个问题：针对 Consumer Group，Kafka 是怎么管理位移的呢？你还记得吧，消费者在消费的过程中需要记录自己消费了多少数据，即消费位置信息。在 Kafka 中，这个位置信息有个专门的术语：位移（Offset）。

看上去该 Offset 就是一个数值而已，其实对于 Consumer Group 而言，它是一组 KV 对，Key 是分区，V 对应 Consumer 消费该分区的最新位移。如果用 Java 来表示的话，你大致可以认为是这样的数据结构，即 Map，其中 TopicPartition 表示一个分区，而 Long 表示位移的类型。当然，我必须承认 Kafka 源码中并不是这样简单的数据结构，而是要比这个复杂得多，不过这并不会妨碍我们对 Group 位移的理解。

我在专栏第 4 期中提到过 Kafka 有新旧客户端 API 之分，那自然也就有新旧 Consumer 之分。老版本的 Consumer 也有消费者组的概念，它和我们目前讨论的 Consumer Group 在使用感上并没有太多的不同，只是它管理位移的方式和新版本是不一样的。

老版本的 Consumer Group 把位移保存在 ZooKeeper 中。Apache ZooKeeper 是一个分布式的协调服务框架，Kafka 重度依赖它实现各种各样的协调管理。将位移保存在 ZooKeeper 外部系统的做法，最显而易见的好处就是减少了 Kafka Broker 端的状态保存开销。现在比较流行的提法是将服务器节点做成无状态的，这样可以自由地扩缩容，实现超强的伸缩性。Kafka 最开始也是基于这样的考虑，才将 Consumer Group 位移保存在独立于 Kafka 集群之外的框架中。

不过，慢慢地人们发现了一个问题，即 ZooKeeper 这类元框架其实并不适合进行频繁的写更新，而 Consumer Group 的位移更新却是一个非常频繁的操作。这种大吞吐量的写操作会极大地拖慢 ZooKeeper 集群的性能，因此 Kafka 社区渐渐有了这样的共识：将 Consumer 位移保存在 ZooKeeper 中是不合适的做法。

于是，在新版本的 Consumer Group 中，Kafka 社区重新设计了 Consumer Group 的位移管理方式，采用了将位移保存在 Kafka 内部主题的方法。这个内部主题就是让人既爱又恨的 __consumer_offsets。我会在专栏后面的内容中专门介绍这个神秘的主题。不过，现在你需要记住新版本的 Consumer Group 将位移保存在 Broker 端的内部主题中。

最后，我们来说说 Consumer Group 端大名鼎鼎的重平衡，也就是所谓的 Rebalance 过程。我形容其为“大名鼎鼎”，从某种程度上来说其实也是“臭名昭著”，因为有关它的 bug 真可谓是此起彼伏，从未间断。这里我先卖个关子，后面我会解释它“遭人恨”的地方。我们先来了解一下什么是 Rebalance。

Rebalance 本质上是一种协议，规定了一个 Consumer Group 下的所有 Consumer 如何达成一致，来分配订阅 Topic 的每个分区。比如某个 Group 下有 20 个 Consumer 实例，它订阅了一个具有 100 个分区的 Topic。正常情况下，Kafka 平均会为每个 Consumer 分配 5 个分区。这个分配的过程就叫 Rebalance。

那么 Consumer Group 何时进行 Rebalance 呢？Rebalance 的触发条件有 3 个。

1。组成员数发生变更。比如有新的 Consumer 实例加入组或者离开组，抑或是有 Consumer 实例崩溃被“踢出”组。
2。订阅主题数发生变更。Consumer Group 可以使用正则表达式的方式订阅主题，比如 consumer.subscribe(Pattern.compile(“t.*c”)) 就表明该 Group 订阅所有以字母 t 开头、字母 c 结尾的主题。在 Consumer Group 的运行过程中，你新创建了一个满足这样条件的主题，那么该 Group 就会发生 Rebalance。
3。订阅主题的分区数发生变更。Kafka 当前只能允许增加一个主题的分区数。当分区数增加时，就会触发订阅该主题的所有 Group 开启 Rebalance。

Rebalance 发生时，Group 下所有的 Consumer 实例都会协调在一起共同参与。你可能会问，每个 Consumer 实例怎么知道应该消费订阅主题的哪些分区呢？这就需要分配策略的协助了。

当前 Kafka 默认提供了 3 种分配策略，每种策略都有一定的优势和劣势，我们今天就不展开讨论了，你只需要记住社区会不断地完善这些策略，保证提供最公平的分配策略，即每个 Consumer 实例都能够得到较为平均的分区数。比如一个 Group 内有 10 个 Consumer 实例，要消费 100 个分区，理想的分配策略自然是每个实例平均得到 10 个分区。这就叫公平的分配策略。如果出现了严重的分配倾斜，势必会出现这种情况：有的实例会“闲死”，而有的实例则会“忙死”。

我们举个简单的例子来说明一下 Consumer Group 发生 Rebalance 的过程。假设目前某个 Consumer Group 下有两个 Consumer，比如 A 和 B，当第三个成员 C 加入时，Kafka 会触发 Rebalance，并根据默认的分配策略重新为 A、B 和 C 分配分区，如下图所示：
【07-配图-Consumer Group 发生 Rebalance 的过程.png】

显然，Rebalance 之后的分配依然是公平的，即每个 Consumer 实例都获得了 2 个分区的消费权。这是我们希望出现的情形。

讲完了 Rebalance，现在我来说说它“遭人恨”的地方。

首先，Rebalance 过程对 Consumer Group 消费过程有极大的影响。如果你了解 JVM 的垃圾回收机制，你一定听过万物静止的收集方式，即著名的 stop the world，简称 STW。在 STW 期间，所有应用线程都会停止工作，表现为整个应用程序僵在那边一动不动。Rebalance 过程也和这个类似，在 Rebalance 过程中，所有 Consumer 实例都会停止消费，等待 Rebalance 完成。这是 Rebalance 为人诟病的一个方面。

其次，目前 Rebalance 的设计是所有 Consumer 实例共同参与，全部重新分配所有分区。其实更高效的做法是尽量减少分配方案的变动。例如实例 A 之前负责消费分区 1、2、3，那么 Rebalance 之后，如果可能的话，最好还是让实例 A 继续消费分区 1、2、3，而不是被重新分配其他的分区。这样的话，实例 A 连接这些分区所在 Broker 的 TCP 连接就可以继续用，不用重新创建连接其他 Broker 的 Socket 资源。

最后，Rebalance 实在是太慢了。曾经，有个国外用户的 Group 内有几百个 Consumer 实例，成功 Rebalance 一次要几个小时！这完全是不能忍受的。最悲剧的是，目前社区对此无能为力，至少现在还没有特别好的解决方案。所谓“本事大不如不摊上”，也许最好的解决方案就是避免 Rebalance 的发生吧。

小结

总结一下，今天我跟你分享了 Kafka Consumer Group 的方方面面，包括它是怎么定义的，它解决了哪些问题，有哪些特性。同时，我们也聊到了 Consumer Group 的位移管理以及著名的 Rebalance 过程。希望在你开发 Consumer 应用时，它们能够助你一臂之力。
【07-配图-Kafka的消费者组.jpg】

开放讨论

今天我貌似说了很多 Consumer Group 的好话（除了 Rebalance），你觉得这种消费者组设计的弊端有哪些呢？

欢迎写下你的思考和答案，我们一起讨论。如果你觉得有所收获，也欢迎把文章分享给你的朋友。

精选留言(71)


耿斌 置顶
“显然，Rebalance 之后的分配依然是公平的，即每个 Consumer 实例都获得了 3 个分区的消费权。”
这里应该是每个Consumer实例都获得了2个分区的消费权
有个问题，Consumer group是可以任意指定创建的？
作者回复: 感谢纠正：）
可以任意指定创建
2019-07-23



电光火石
如何避免rebalance发生？我发现线上在没有这三种情况也会发生，我猜是网络瞬断导致的，但不知道kafka是否会发生定时的rebalance？谢谢了
作者回复: 网络断了，心跳中断，consumer被踢出组，也属于第一种情况
2019-07-08


6

October
消费组中的消费者个数如果超过topic的分区数，就会有消费者消费不到数据。但如果是同一个消费组里的两个消费者通过assign方法订阅了同一个TopicPartition，是不是会有一个消费者不能消费到消息？
作者回复: 如果使用assign，则表明该consumer是独立consumer（standalone consumer），它不属于任何消费者组。独立consumer可以订阅任何分区，彼此之间也没有关系，即两个独立consumer可以订阅并消费相同的分区
2019-07-06


6

QQ怪
老师最后那个Rebalance例子有问题吧，最后每个consumer只有两个而不是三个吧
作者回复: 嗯嗯，不就是每个consumer分配两个吗？
2019-07-06

2

6

nightmare
什么时候来个consumer端手动管理offset的方案
2019-07-06

4

6

永光
发布 / 订阅模型倒是允许消息被多个 Consumer 消费，但它的问题也是伸缩性不高，因为每个订阅者都必须要订阅主题的所有分区。这种全量订阅的方式既不灵活，也会影响消息的真实投递效果。
问题：
1、每个订阅者都必须要订阅主题的所有分区，是否意味着每个订阅者都需要消费所有的分区的所有消息？
2、我理解一个主题下进行分区消费就可以满足日需求了，Consumer Group为什么设计成可以订阅多个主题，什么样的场景会使订阅多个主题？
谢谢。
作者回复: 1. 不会。每个订阅者分配一部分分区消费
2. 没有什么规定要求什么场景下需要订阅多个主题。事实上，对于默认的分区策略，一个组订阅多个主题的做法会导致分配的极不均匀，但我们依然还是能够找出一些场景，使得这么做是有意义的。比如消费者组订阅多组传感器的数据，我们不确定未来新增传感器的主题名到底是什么，但可以约定所有传感器的主题名以sensor开头，那么此时让group订阅以sensor开头的所有主题就能动态地检测后续新增的主题。这个场景是不是有意义些？
2019-07-11


5

东方奇骥
难得一个双休，今天终于学习到这篇了，好想实战上手玩一玩Kafka。老师，最后一个章节才会有实战Demo吗？
2019-07-06


5

Tony Du
请老师讲讲手动管理consumer offset的工程实践
作者回复: 专栏后面有专门的内容：）
2019-07-06


5

骨汤鸡蛋面
我们现在服务是3个topic，每个topic有64个分区，6个消费实例，每次服务重新部署（6个实例依次关闭启动）都会导致长时间的rebalance，是否减少topic的分区数可以减少服务部署时rebalance的时间呢？
作者回复: 嗯，会的。减少consumer个数也有缩短rebalance。
2019-07-06


4

张珮磊想静静
会不会存在这样一个情况：一个consumer正在消费一个分区的一条消息，还没有消费完，发生了rebalance(加入了一个consumer)，从而导致这条消息没有消费成功，rebalance后，另一个consumer又把这条消息消费一遍
作者回复: 非常可能存在
2019-07-31

1

3

Xiao
老师，rebalance的时候可以不可以这样做，如果是consumer挂掉导致，那不做group的rebalance，仅仅是将挂掉节点上的partition重新分配给别的consume让，只有在consumer消费特别不均匀的情况下才做group的rebalance；
如果是添加节点导致rebalance，那也不用一次性就做，可以分阶段，比如说先把消费压力大的consumer上的partition分一部分给新进来的consumer！
2019-07-08

1

2

☆appleう
举个简单的例子，假设一个 Consumer Group 订阅了 3 个主题，分别是 A、B、C，它们的分区数依次是 1、2、3，那么通常情况下，为该 Group 设置 6 个 Consumer 实例是比较理想的情形，因为它能最大限度地实现高伸缩性。

你可能会问，我能设置小于或大于 6 的实例吗？当然可以！如果你有 3 个实例，那么平均下来每个实例大约消费 2 个分区（6 / 3 = 2）；如果你设置了 8 个实例，那么很遗憾，有 2 个实例（8 – 6 = 2）将不会被分配任何分区，它们永远处于空闲状态。因此，在实际使用过程中一般不推荐设置大于总分区数的 Consumer 实例。设置多余的实例只会浪费资源，而没有任何好处。


老师，针对上面这段例子有一个问题: 如果8个实例，6个分区，每个实例负责分配一个分区，多出来的2个实例不能与其他消费实例共同负责一个分区吗？
作者回复: 如果它们都属于同一个消费者组，那么不能。消费者组订阅的分区只能被组内一个consumer实例消费。
2019-07-07


2

maben996
文中提到，同一个Group中的不同Consumer实例负责消费Topic的不同分区。
有一个问题，同一个Group中的不同Consumer实例可以订阅不同的Topic吗？
作者回复: 可以的。虽然在实际使用中可能更多的还是同一个group的多个实例订阅相同的topic。
2019-10-22


1

Ryan
胡老师好，有一个rebalance的问题请教一下：
假设一个topic有P1和P2两个分区， 由同一消费者组中的C1和C2消费， C1消费P1, C2消费P2。
C1和C2都是手动管理offset的，消息消费后，会将offset保存到mysql中，然后commit到broker
某一时刻，C1 offset到100 消费完成后保存mysql, 但是没有调用commitSync()
C2 offset到200, 消费完成后保存offset到mysql, 但是没有调用commitSync()
然后发生了rebalance， rebalance后，C1分配到P2分区，C2分配到P1分区。
之后，C1和C2调用commitSync(), （其实是commit rebalance前的分区的offset）,会发生什么情况？
作者回复: 如果你没有实现ConsumerRebalanceListener接口中的方法显式从MySQL中恢复offset，那么rebalance回来后会从Broker中读取Kafka这边保存的已知的最新位移，但很可能不是你MySQL中保存的位移，因此此时调用commitSync没有什么效果。
2019-10-14


1

北冥Master
elasticsearch在增加分片和减少分片时也有类似的重平衡的过程。es需要迁移数据所以时间会很长，kafka又不用迁移数据为什么要几个小时?看起来只是分区ID重新哈希一下?耗时主要在哪里？
作者回复: kafka增加了分区手动迁移的话也要迁移数据的
2019-08-26

2

1

godtrue
老师好，我有以下几个疑问，请帮忙解答一下，多谢!
1：请问老师，什么场景下消费者组中的一个实例会是一个进程中的线程呢？

2：传统的消息队列模型的缺陷在于消息一旦被消费，就会从队列中被删除，而且只能被下游的一个 Consumer 消费。
请问老师，这里描述的删除操作，具体是什么操作？将消息从队列中移出？还是移动位移标识此位置可以使用？这些操作应该是内存中的吧！对于落盘的消息以及副本中的消息也有删除的动作吧？删除一个消息的完整流程是怎么样的呢？

3：发布 / 订阅模型倒是允许消息被多个 Consumer 消费，但它的问题也是伸缩性不高，因为每个订阅者都必须要订阅主题的所有分区。这种全量订阅的方式既不灵活，也会影响消息的真实投递效果。
如果一个主题可以有多个分区，也能增加分区数据，如果消费能力不足，可以增加消费者，这种方式没理解怎么伸缩性就不高了？不是一个consumer实例只能消费主题下的一个分区嘛？
作者回复: 1. 如果你的client端机器非常强劲，只启动一个consumer实例单线程消费未免有些浪费，你可以以启动多个线程的方式来充分利用资源。
2. 消息被删除。具体流程因不同框架而定
3. 1个consumer必须订阅所有分区，这是不必要的
2019-08-15


1

老鱼
rebalance时，全部实例都要参与重新分配。是否能参考 一致性哈希算法，尽量减少对全局的影响？
作者回复: 是个不错的思路：）
2019-07-24


1

牧码人
想请问一下consumer group可以删除吗？或者有其他命令管理吗？
作者回复: 可以删除。使用kafka-consumer-groups命令
2019-07-08


1

愚人
订阅topic是在消费者程序中实现的，如果一个group内多个消费者分别订阅了不同的topic，是不是所有这些topic下的全部分区都会统一分配这个group内的消费者？比如group内消费者A只订阅topic1，但是topic2（被另一个消费者订阅）下的某一个分区却分配给了消费者A？
作者回复: 不会的。分配的原则还是要考虑每个consumer的订阅情况。不可能把你没订阅的分区分给你
2019-11-30



man1s
那我如果有100个分区，100个同组消费者，在启动这100个消费者过程中会发生100次rebalace吗
作者回复: 目前不会
2019-11-19



James
请问下老师,单消费实例在gc下会出现重平衡吗.
还是因为重平衡导致gc出现
作者回复: 可能出现gc导致rebalance的
2019-11-14

1


Geek_zy
老师 既然kafka 曾经独立出zk来存储consumer group 的offset 是有好处的，仅仅因为zk不太适合做频繁的更新，才又整合到kafka中，为什么不考虑另外一种工具适合做频繁的更新操作去存储它。
作者回复: 社区希望对所有的东西都能有掌控度，而不是依赖其他第三方组件
2019-11-12



惜昔
请问老师Consumer group是第一个消费者订阅的时候创建的吗？后面订阅该topic的消费者会不会自动加入该消费者组
作者回复: “Consumer group是第一个消费者订阅的时候创建的吗” --- 我觉得你可以这么理解。

“后面订阅该topic的消费者会不会自动加入该消费者组” --- 如果这个消费者设置了相同的groupID，那么不论是否订阅该topic，都会属于这个消费者组。
2019-11-07



注定非凡
好的，老师😁
2019-11-03



注定非凡
Consumer Group ：Kafka提供的可扩展且具有容错性的消息者机制。

1，重要特征：
A：组内可以有多个消费者实例（Consumer Instance）。
B：消费者组的唯一标识被称为Group ID，组内的消费者共享这个公共的ID。
C：消费者组订阅主题，主题的每个分区只能被组内的一个消费者消费
D：消费者组机制，同时实现了消息队列模型和发布/订阅模型。

2，重要问题：
A：消费组中的实例与分区的关系：
消费者组中的实例个数，最好与订阅主题的分区数相同，否则多出的实例只会被闲置。一个分区只能被一个消费者实例订阅。
B：消费者组的位移管理方式：
（1）对于Consumer Group而言，位移是一组KV对，Key是分区，V对应Consumer消费该分区的最新位移。
（2）Kafka的老版本消费者组的位移保存在Zookeeper中，好处是Kafka减少了Kafka Broker端状态保存开销。但ZK是一个分布式的协调框架，不适合进行频繁的写更新，这种大吞吐量的写操作极大的拖慢了Zookeeper集群的性能。
（3）Kafka的新版本采用了将位移保存在Kafka内部主题的方法。

C：消费者组的重平衡：
（1）重平衡：本质上是一种协议，规定了消费者组下的每个消费者如何达成一致，来分配订阅topic下的每个分区。
（2）触发条件：
a，组成员数发生变更
b，订阅主题数发生变更
c，定阅主题分区数发生变更
（3）影响：
Rebalance 的设计是要求所有consumer实例共同参与，全部重新分配所有用分区。并且Rebalance的过程比较缓慢，这个过程消息消费会中止。
作者回复: 专栏结束了把你的分享笔记share出来吧：）
2019-11-02



十字路口发传单
想问一下目前新版本的分组，在没有消费者的情况下如何删除分组
作者回复: 您指的分组是指consumer group吗？如果是，最新版本的AdminClient支持deleteConsumerGroups方法
2019-10-24



Kong
三个主题，三个分区不应该是9个消费者实例嘛，为什么是6个。
作者回复: 分别是 A、B、C，它们的分区数依次是 1、2、3，6个分区啊。可能没表述清楚
2019-10-17

1


😈😈😈😈😈
消费者组中的每个消费者实例，只能消费一部分数据，这样会导致导致单个消费者实例获取的数据不全！！这样不会有问题吗？
作者回复: 单个消费者获取数据不全，但是所有消费者聚在一起就全了啊，这有什么问题吗？
2019-10-08



肥low
我唯一能想到的消费组的弊端就是 一条消息能被消费多次，多个消费组解消息的代码部分是重复^_^
2019-10-02



谢特
Kafka Schema Registry这个可以讲一下吗
作者回复: 嗯嗯，这个是Confluent Kafka提供的功能，Apache Kafka是没有的。咱们这个专栏还是主要覆盖社区版Kafka的功能。
2019-09-04

1


giantbroom
我认为当消费者实例说多于分区数是，多余的实例处于空闲的设计是有问题的，应该能够参与消费分区消息，这样能够极大提高TPS，当然前提是消费端是无状态的。另外关于rebalance有一点不解，按照目前的逻辑，岂不是要求任何一个消费者实例有能力处理所有topic的逻辑，这岂不是很不合理？
作者回复: 目前Kafka消费的最小粒度就是分区，因此一旦consumer实例数大于分区数，必然有空闲consumer的。你的后一个问题没有太明白。。。
2019-09-02

1


DFighting
老师可以讲解下rebanlance会涉及到哪些操作吗？如此重的一个操作kafka为什么要这么设计是有什么非做不可的原因吗？比如最简单的，消费组增加减少，主题增加减少和分区增加减少对应的都是一个消费组中每个消费实例所需要处理的分区数的变化，完全可以通过对各个主题分区的分配策略进行操作来实现均衡消费和高性能，这个操作代价感觉不是那么重量级啊？生产环境中没用过kafka，所以只是自己的一些想法，希望老师能够帮忙指正。
作者回复: 非做不可，毕竟要重新分配分区。只是的确如你所说，并不是所有情况下都要触发rebalance。Rebalance时各个consumer成员要重新加入组，然后等待leader consumer成员制定消费分配方案，之后每个consumer成员拿到自己的方案后开始正常消费。
2019-09-02



DC
Rebalance无法避免，又很慢，如果只是站在使用者的角度看的话，那这kafka怎么感觉很不行啊，在考虑技术栈的时候难道放弃它？
作者回复: 社区2.3引入了static consumer，这样consumer程序正常的停机重启不会rebalance，值得一试：）
2019-08-06



锋芒
老师，请教一下： 一个分区 同一时间段 只会有一个consumer 在消费消息对吗 ？
作者回复: 在消费者组机制下是这样的
2019-08-06



Nic-愛
broker发生变化也是同样会发生reblance吧，虽然和consumer数目变化一样
作者回复: 不会的
2019-07-31

1


金hb.Ryan 冷空氣駕到
@楼主，我看一下 应该是统一配置成roundrobin 吧
2019-07-28



早起的鸟儿
举个简单的例子，假设一个 Consumer Group 订阅了 3 个主题，分别是 A、B、C，它们的分区数依次是 1、2、3，那么通常情况下，为该 Group 设置 6 个 Consumer 实例是比较理想的情形，因为它能最大限度地实现高伸缩性。
老师你好，这个地方没太理解，每个主题只有3个分区，为什么要设置6个consumer实例，如果每个consumer实例消费主题的一个分区，3个分区只需要3个consumer实例，这样不是会多出3个consumer实例吗？还望解答，谢谢
作者回复: 可能没写清楚。我是说有3个主题，它们的分区数分别是1，2，3，总共6个分区。。。
2019-07-26

1


百越平民
老师，是不是同一个group内，同一个topic，同一个分区，最终只会被一个consumer消费（不论是进程还是线程）
作者回复: 是的。
2019-07-25



金hb.Ryan 冷空氣駕到
前面打错了。。。前两天我们一个场景‘一个topic八个partition，一个consumergroup三台服务器，原先是默认range的分配，可改成roundrobin分配以后一直在做rebalance，改回range又好了，kafka版本0.10，有遇到过这种场景么，这个consumergroup还在消费另外两个topic数据不知道有没有关系
作者回复: 是否所有的consumer都统一使用了roundrobin?
2019-07-25



金hb.Ryan 冷空氣駕到
请问 像kafka flink消费程序，offset保存在hdfs中是不是避免rebalance？
作者回复: offset保存在哪和rebalance关系不大的
2019-07-23



金hb.Ryan 冷空氣駕到
前两天我们一个场景‘一个topic八个partition，一个consumergroup三台服务器，原先是默认range的分配，可改成range分配以后一直在做rebalance，改回range又好了，kafka版本0.10，有遇到过这种场景么
作者回复: “原先是默认range的分配，可改成range分配以后一直在做rebalance，改回range又好了”。。。 我有点凌乱了
2019-07-23



信信
前面有同学提问--老师：今天试了下，同一个消费者组，不同进程下的消费者实例，消费同一个主题，只有一个进程能够消费，而在多线程下，却是可以消费同一主题，不同分区下的消息，是这样的吗？
感觉老师理解错问题了。 我这实验结果是：不同进程的消费者组用同一个groupID，确实随机到其中的一个进程消费。 多线程的情况没有试。
作者回复: 不是。多个进程的consumer，只要有相同的groupID，就能参与主题所有分区的消费。主要是看KafkaConsumer实例的个数
2019-07-16



pain
消费者 group 订阅多个 topic 的情况多吗，感觉订阅多个 topic 的话，处理逻辑不是一样的
作者回复: 不能说没有，但我见到的不多
2019-07-16



星期八
老师：今天试了下，同一个消费者组，不同进程下的消费者实例，消费同一个主题，只有一个进程能够消费，而在多线程下，却是可以消费同一主题，不同分区下的消息，是这样的吗？
作者回复: 应该不是这样的。只要是多个KafkaConsumer实例配置了相同的group.id，就应该能够同时消费不同的主题分区
2019-07-12



趁早
写的很透彻的一门kafka的专栏
2019-07-12



Algoric
看了重平衡，希望大佬后面能够讲下副本在broker上具体如何分配，以及生产环境下分区平衡怎么实现
2019-07-10



丘壑
以前搞过一个车联网项目，最初就是采用的指定分区进行消费，但是由于消息量太大，导致了消费者实例有时出现了问题，比如内存溢出，程序半夜可能出现问题，这个时候由于各种原因处理不了，这样就导致了某几个分区没有消费者消费，当时就想改造为按group的方式进行消费，借助kafka-Rebalance可以有其他消费者顶替，减少运维负担，按照本讲中的思想，这种大数据量的消息场景完全不能使用group的Rebalance进行消费啊，这。。。。。。。忘解惑，
2019-07-10



丘壑
看完更困惑了，那依据老师的观点，就是在多分区，多消费者的情况下，就不建议使用kafka-group的方式进行消费了，而是手动指定消费分区避免rebalance，这种多分区多消费者的场景不就是消息量非常大的场景吗？这不就是kafka的优势项目吗？难道kafka的优势在大量消息的场景就这么不堪吗？求大厂真相
作者回复: group和standalone consumer都可以适用于大体量的消费啊。使用Group仅仅是为了利用它的自动分区分配功能。不是说group方式的consumer性能就要好或不好
 
2019-07-10



末北。
老师你好，如果一个topic的consumer产生变化，那么进行重平衡的时候，只是这一个topic发生重平衡还是所有topic都会发生重平衡，这时候所有的消息都不能消费是吗？等重平衡结束才能再次消费吗？
作者回复: 重平衡是指consumer group级别的，不是主题级别的。Rebalance时所有consumer都不能消费，等结束后才能继续消费
2019-07-10



巧克力黑
老师，你好
新版本位移保存在内部主题。这里说的新版本，是从哪个版本开始的？
作者回复: 0.9版本开始的
2019-07-09



巧克力黑
老师，你好新版本位移保存在 Kafka 内部主题, __consumer_offsets。这个
2019-07-09



末北。
老师请问我的程序经常出现partitions revoked:这种会是什么原因导致的那
作者回复: rebalance时会回收所有consumer负责的分区，也就是所谓的revoked。查一下为什么会频繁地出现rebalance吧
2019-07-09



双叶
最后的例子中，如果不同 topic 的流量差异很大，那么就可能会造成机器之间负载差异较大？
2019-07-09



大倪(Xin Jun)
老师可以说说如何避免rebalance，比如设置什么参数来延长心跳时间，避免被错误踢掉导致频繁rebalance
作者回复: 专栏后面有专门讨论如何避免rebalance：）
2019-07-09



玉剑冰锋
请教老师两个问题1.关于删除topic，由于采集端使用filebeat(线上几百台)，现在发现topic异常，我想删除重新创建，由于filebeat不停地在写入数据，删除后立刻创建出来导致无法删除，想问一下如果我不停止filebeat有没有其他办法彻底删除;2.关于在线迁移或者添加副本，使用kafka-reassign-partitions.sh添加topic副本，分区数据量很小，持续好几天了一直还有in progress，这是什么原因导致的，另外通过什么参数可以调整迁移速率，谢谢
作者回复: 1. 几乎没办法。。。。
2. 失败原因只能看日志，通常都是因为topic被删除了或有分区不可用导致的。该命令提供了throttle来限速，可以一用。我们在实际生产环境中也用了这个来限制贷款
2019-07-09



WL
请问老师有啥好办法避免发生rebalance呢？感觉热rebalance的触发条件很容易发生啊，消费者组中的一台服务器出问题不就rebalance了，那整个组不可用了不是变相的把问题扩大化了吗？
作者回复: 好好设置max.poll.interval.ms的值。实在不行可以尝试使用standalone consumer
2019-07-09



bunny
胡老师，在你举的rebalance好几个小时的例子中，假如事先知道是增加实例还是减少实例，那么这样做是不是好些：先停止所有消费者实例，然后再启动！当然如果是其他未知的因素导致rebalance，那就控制不了了
2019-07-09



空知
老师请教下
1、消费者位移 是记录 分区和偏移量的 没有记录是哪个消费者,这样的话下一个消费者来消费这个分区时候 偏移量是重置的嘛?
2、多个group订阅同一个topic时候 不同group可以同时访问一个leader分区嘛?
作者回复: 1. 不会重置，如果这个分区由组内其他consumer分配了，也是接着之前的进度继续消费。
2. 可以
2019-07-08

1


莫
老师，使用kafka 的时候，发现消费者进程依然存在，但是不进行消费，这会是什么问题导致
作者回复: 可能的原因太多了。。。最好给出详细的描述
2019-07-08



yyyiue
rebalance需要客户端编码支持吗？
2019-07-08



明翼
老师，请教下，这个保存位移的分区有多少个分区多少个副本那？我们可以设置吗？
作者回复: 默认560个分区，3个副本。可以设置也可以自行创建
2019-07-08

1


明翼
我觉得不好地方：一是限制了并发度消费者个数最大只能和kafka的分数数一样；如果kafka的数据再分区中不平衡就导致了只能等一个消费者慢慢消费；多个消费者组那就要保存多个消费组的位移信息，这个也蛮坑的，那个消费组位移的topic我记得是不是也有坑。
2019-07-08



Geek_jacky
老师好，多进程同一个消费组去消费同一个topic，会出现有的进程消费不到的问题。同时一个进程多个线程消费同一个topic是可以的，那这个多线程与多进程同消费组消费同一个topic这两个之间有什么不一样？
作者回复: 没有什么不同，至少对于Kafka而言
2019-07-08



外星人
那也就是说__consumer_offset这个topic和其他的topic做reassign的时候可以一样对待，没有特殊需要考虑或注意的地方是吗？consumer的groupcoordinator不是根据partition leader所在的broker来确定的吗？做reassign会不会影响消费端啊？
作者回复: 会影响位移提交啊
2019-07-08



曹操
请问一下老师，kafka有基于时间戳的消费方式吗？比如我想从某个时间点之后投入kafka的数据开始消费？
作者回复: 有，KafkaConsumer有offsetsForTimes方法
2019-07-08

1


老醋
不行吧，Kafka为了消除多个consumer消费同一分区进行的同步操作，组内不同consumer不能消费同一分区
2019-07-08



蒙开强
老师，你好，怎么设置消费组里的消费实例个数呢
作者回复: 没有一个参数来设置此事。主要看启动了多少个具有相同group.id的consumer实例。
2019-07-07



外星人
老师，你好，我们最近集群扩容，需要对__consumer_offset这个topic进行reassign，请问有没有坑啊？需要注意哪些事项呢？期待您的回答，谢谢
作者回复: reassign操作很容易出错，不只是对__consumer_offsets。我个人的建议哈：1. 业务低峰时段做；2. 不要topic级别整体迁移，最好按照分区级别来做。比如一次迁移几个分区这样
2019-07-07



October
一个TopicPatition的消息只能被消费组的某个消费者所消费，当出现数据倾斜时（比如某个key的消息的特别多就会导致数据倾斜），可能一个消费者的的消费速度不足以应付业务对消息处理延迟的要求，不知道这算不算一个缺点。请老师指正。
作者回复: 嗯。如果单个分区依然数据很多，可以考虑多加一些分区，将数据分散到更多的分区上，降低单个分区的负载。
2019-07-06

1


风中花
看的我都stop the word ！一口气看完！看觉一下子吸收了不少功法套路！
2019-07-06



wykkx
“在新版本的 Consumer Group 中，Kafka 社区重新设计了 Consumer Group 的位移管理方式”老师 请问这是在哪个版本以后算是你 说的新版本，这里要如何进行配置是使用内部的还是使用外部的zk集群，在新版本里。
作者回复: 0.9版本引入了新版本consumer。新版本将位移保存在__consumer_offsets中，不需要额外配置。如果你想保存在外部的Zk集群中，那么设置consumer端参数enable.auto.commit=false，然后自己调用ZooKeeper API提交位移到Zk即可。
2019-07-06

2
