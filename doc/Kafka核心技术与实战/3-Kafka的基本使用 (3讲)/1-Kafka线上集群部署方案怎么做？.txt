专栏前面几期内容，我分别从 Kafka 的定位、版本的变迁以及功能的演进等几个方面循序渐进地梳理了 Apache Kafka 的发展脉络。通过这些内容，我希望你能清晰地了解 Kafka 是用来做什么的，以及在实际生产环境中该如何选择 Kafka 版本，更快地帮助你入门 Kafka。

现在我们就来看看在生产环境中的 Kafka 集群方案该怎么做。既然是集群，那必然就要有多个 Kafka 节点机器，因为只有单台机器构成的 Kafka 伪集群只能用于日常测试之用，根本无法满足实际的线上生产需求。而真正的线上环境需要仔细地考量各种因素，结合自身的业务需求而制定。下面我就分别从操作系统、磁盘、磁盘容量和带宽等方面来讨论一下。

操作系统

首先我们先看看要把 Kafka 安装到什么操作系统上。说起操作系统，可能你会问 Kafka 不是 JVM 系的大数据框架吗？Java 又是跨平台的语言，把 Kafka 安装到不同的操作系统上会有什么区别吗？其实区别相当大！

的确，如你所知，Kafka 由 Scala 语言和 Java 语言编写而成，编译之后的源代码就是普通的“.class”文件。本来部署到哪个操作系统应该都是一样的，但是不同操作系统的差异还是给 Kafka 集群带来了相当大的影响。目前常见的操作系统有 3 种：Linux、Windows 和 macOS。应该说部署在 Linux 上的生产环境是最多的，也有一些 Kafka 集群部署在 Windows 服务器上。Mac 虽然也有 macOS Server，但是我怀疑是否有人（特别是国内用户）真的把生产环境部署在 Mac 服务器上。

如果考虑操作系统与 Kafka 的适配性，Linux 系统显然要比其他两个特别是 Windows 系统更加适合部署 Kafka。虽然这个结论可能你不感到意外，但其中具体的原因你也一定要了解。主要是在下面这三个方面上，Linux 的表现更胜一筹。

I/O 模型的使用
数据网络传输效率
社区支持度

我分别来解释一下，首先来看 I/O 模型。什么是 I/O 模型呢？你可以近似地认为 I/O 模型就是操作系统执行 I/O 指令的方法。

主流的 I/O 模型通常有 5 种类型：阻塞式 I/O、非阻塞式 I/O、I/O 多路复用、信号驱动 I/O 和异步 I/O。每种 I/O 模型都有各自典型的使用场景，比如 Java 中 Socket 对象的阻塞模式和非阻塞模式就对应于前两种模型；而 Linux 中的系统调用 select 函数就属于 I/O 多路复用模型；大名鼎鼎的 epoll 系统调用则介于第三种和第四种模型之间；至于第五种模型，其实很少有 Linux 系统支持，反而是 Windows 系统提供了一个叫 IOCP 线程模型属于这一种。

你不必详细了解每一种模型的实现细节，通常情况下我们认为后一种模型会比前一种模型要高级，比如 epoll 就比 select 要好，了解到这一程度应该足以应付我们下面的内容了。

说了这么多，I/O 模型与 Kafka 的关系又是什么呢？实际上 Kafka 客户端底层使用了 Java 的 selector，selector 在 Linux 上的实现机制是 epoll，而在 Windows 平台上的实现机制是 select。因此在这一点上将 Kafka 部署在 Linux 上是有优势的，因为能够获得更高效的 I/O 性能。

其次是网络传输效率的差别。你知道的，Kafka 生产和消费的消息都是通过网络传输的，而消息保存在哪里呢？肯定是磁盘。故 Kafka 需要在磁盘和网络间进行大量数据传输。如果你熟悉 Linux，你肯定听过零拷贝（Zero Copy）技术，就是当数据在磁盘和网络进行传输时避免昂贵的内核态数据拷贝从而实现快速地数据传输。Linux 平台实现了这样的零拷贝机制，但有些令人遗憾的是在 Windows 平台上必须要等到 Java 8 的 60 更新版本才能“享受”到这个福利。一句话总结一下，在 Linux 部署 Kafka 能够享受到零拷贝技术所带来的快速数据传输特性。

最后是社区的支持度。这一点虽然不是什么明显的差别，但如果不了解的话可能比前两个因素对你的影响更大。简单来说就是，社区目前对 Windows 平台上发现的 Kafka Bug 不做任何承诺。虽然口头上依然保证尽力去解决，但根据我的经验，Windows 上的 Bug 一般是不会修复的。因此，Windows 平台上部署 Kafka 只适合于个人测试或用于功能验证，千万不要应用于生产环境。

磁盘

如果问哪种资源对 Kafka 性能最重要，磁盘无疑是要排名靠前的。在对 Kafka 集群进行磁盘规划时经常面对的问题是，我应该选择普通的机械磁盘还是固态硬盘？前者成本低且容量大，但易损坏；后者性能优势大，不过单价高。我给出的建议是使用普通机械硬盘即可。

Kafka 大量使用磁盘不假，可它使用的方式多是顺序读写操作，一定程度上规避了机械磁盘最大的劣势，即随机读写操作慢。从这一点上来说，使用 SSD 似乎并没有太大的性能优势，毕竟从性价比上来说，机械磁盘物美价廉，而它因易损坏而造成的可靠性差等缺陷，又由 Kafka 在软件层面提供机制来保证，故使用普通机械磁盘是很划算的。

关于磁盘选择另一个经常讨论的话题就是到底是否应该使用磁盘阵列（RAID）。使用 RAID 的两个主要优势在于：

提供冗余的磁盘存储空间
提供负载均衡

以上两个优势对于任何一个分布式系统都很有吸引力。不过就 Kafka 而言，一方面 Kafka 自己实现了冗余机制来提供高可靠性；另一方面通过分区的概念，Kafka 也能在软件层面自行实现负载均衡。如此说来 RAID 的优势就没有那么明显了。当然，我并不是说 RAID 不好，实际上依然有很多大厂确实是把 Kafka 底层的存储交由 RAID 的，只是目前 Kafka 在存储这方面提供了越来越便捷的高可靠性方案，因此在线上环境使用 RAID 似乎变得不是那么重要了。综合以上的考量，我给出的建议是：

追求性价比的公司可以不搭建 RAID，使用普通磁盘组成存储空间即可。
使用机械磁盘完全能够胜任 Kafka 线上环境。

磁盘容量

Kafka 集群到底需要多大的存储空间？这是一个非常经典的规划问题。Kafka 需要将消息保存在底层的磁盘上，这些消息默认会被保存一段时间然后自动被删除。虽然这段时间是可以配置的，但你应该如何结合自身业务场景和存储需求来规划 Kafka 集群的存储容量呢？

我举一个简单的例子来说明该如何思考这个问题。假设你所在公司有个业务每天需要向 Kafka 集群发送 1 亿条消息，每条消息保存两份以防止数据丢失，另外消息默认保存两周时间。现在假设消息的平均大小是 1KB，那么你能说出你的 Kafka 集群需要为这个业务预留多少磁盘空间吗？

我们来计算一下：每天 1 亿条 1KB 大小的消息，保存两份且留存两周的时间，那么总的空间大小就等于 1 亿 * 1KB * 2 / 1000 / 1000 = 200GB。一般情况下 Kafka 集群除了消息数据还有其他类型的数据，比如索引数据等，故我们再为这些数据预留出 10% 的磁盘空间，因此总的存储容量就是 220GB。既然要保存两周，那么整体容量即为 220GB * 14，大约 3TB 左右。Kafka 支持数据的压缩，假设压缩比是 0.75，那么最后你需要规划的存储空间就是 0.75 * 3 = 2.25TB。

总之在规划磁盘容量时你需要考虑下面这几个元素：

新增消息数
消息留存时间
平均消息大小
备份数
是否启用压缩

带宽

对于 Kafka 这种通过网络大量进行数据传输的框架而言，带宽特别容易成为瓶颈。事实上，在我接触的真实案例当中，带宽资源不足导致 Kafka 出现性能问题的比例至少占 60% 以上。如果你的环境中还涉及跨机房传输，那么情况可能就更糟了。

如果你不是超级土豪的话，我会认为你和我平时使用的都是普通的以太网络，带宽也主要有两种：1Gbps 的千兆网络和 10Gbps 的万兆网络，特别是千兆网络应该是一般公司网络的标准配置了。下面我就以千兆网络举一个实际的例子，来说明一下如何进行带宽资源的规划。

与其说是带宽资源的规划，其实真正要规划的是所需的 Kafka 服务器的数量。假设你公司的机房环境是千兆网络，即 1Gbps，现在你有个业务，其业务目标或 SLA 是在 1 小时内处理 1TB 的业务数据。那么问题来了，你到底需要多少台 Kafka 服务器来完成这个业务呢？

让我们来计算一下，由于带宽是 1Gbps，即每秒处理 1Gb 的数据，假设每台 Kafka 服务器都是安装在专属的机器上，也就是说每台 Kafka 机器上没有混布其他服务，毕竟真实环境中不建议这么做。通常情况下你只能假设 Kafka 会用到 70% 的带宽资源，因为总要为其他应用或进程留一些资源。

根据实际使用经验，超过 70% 的阈值就有网络丢包的可能性了，故 70% 的设定是一个比较合理的值，也就是说单台 Kafka 服务器最多也就能使用大约 700Mb 的带宽资源。

稍等，这只是它能使用的最大带宽资源，你不能让 Kafka 服务器常规性使用这么多资源，故通常要再额外预留出 2/3 的资源，即单台服务器使用带宽 700Mb / 3 ≈ 240Mbps。需要提示的是，这里的 2/3 其实是相当保守的，你可以结合你自己机器的使用情况酌情减少此值。

好了，有了 240Mbps，我们就可以计算 1 小时内处理 1TB 数据所需的服务器数量了。根据这个目标，我们每秒需要处理 2336Mb 的数据，除以 240，约等于 10 台服务器。如果消息还需要额外复制两份，那么总的服务器台数还要乘以 3，即 30 台。

怎么样，还是很简单的吧。用这种方法评估线上环境的服务器台数是比较合理的，而且这个方法能够随着你业务需求的变化而动态调整。

小结

所谓“兵马未动，粮草先行”。与其盲目上马一套 Kafka 环境然后事后费力调整，不如在一开始就思考好实际场景下业务所需的集群环境。在考量部署方案时需要通盘考虑，不能仅从单个维度上进行评估。相信今天我们聊完之后，你对如何规划 Kafka 生产环境一定有了一个清晰的认识。现在我来总结一下今天的重点：
【1-配图-小结.jpeg】

开放讨论

对于今天我所讲的这套评估方法，你有什么问题吗？你还能想出什么改进的方法吗？

欢迎你写下自己的思考或疑问，我们一起讨论 。如果你觉得有所收获，也欢迎把文章分享给你的朋友。

精选留言(63)


mickle 置顶
1000*1000/(60*60)=277,这个2336MB是怎么换算来的，还有什么要考虑的吗?
作者回复: 277MB，乘以8，大致等于2300+Mb（小b）。带宽资源一般用Mbps而不是MBps衡量
2019-06-15

2

24

A_NATE_👻
我们曾经也认为用普通硬盘就行，换成普通硬盘导致生产者堵塞写入负载偏高，换成SSD就没事了，我们每天消息数大概50亿。
作者回复: 嗯嗯，专栏里面只是给出一个评估的方法。具体还要结合自己的实际情况来调整。通常我们认为SSD的顺序写TPS大约是HDD的4倍。除了纵向扩展使用SSD之外，也可以尝试一下横向扩展，增加更多的broker或HDD分散负载：）
2019-06-15

3

15

南辕北辙
这个假设是：follower与leader处于不同的broker而实际环境中不推荐单机多broker的架构 摘自老师回复其他同学。
老师这个的意思是不是生产上的架构通常一台服务器上只会有leader或者follow的分区，而不会二者存在一台服务器上，所以根据带宽计算服务器数量时，根据备份数为2，所以就直接✖️3了。
作者回复: Leader副本和Follower副本必然在不同的Broker上，而生产环境一般也不推荐将多台Broker混布到同一台服务器上。当然服务器性能强劲的话也未尝不可：）
2019-06-17


6

Geek_Sue
胡老师，您好，我想请问下，我们公司的环境是基于Docker这种微服务架构，那么kafka部署在Docker容器中部署方案是否会有一些不同呢？
作者回复: 目前社区对Docker方案支持的并不是太好，主要都是一些第三方公司还有Confluent公司在提供解决方案。在Docker上部署我个人觉得没有太大的不同，只是注意带宽资源吧，因为常见的做法都是买一台性能超强的服务器然后在上面启动多个Docker容器，虽然CPU、RAM、磁盘都能承受，但单机还是受限于带宽的。
2019-06-17


5

蒙开强
老师，你好，你讲的这几个纬度很好，之前我们搭建一套kafka集群就不知道怎么去衡量，我再问一个相关问题，我个人觉得kafka会出现丢数据情况，比如某个分区的leader挂了，在切换选举到另外副本为leader时，这个副本还没同步之前的leader数据，这样数据就丢了
作者回复: 嗯嗯，对于producer而言，如果在乎数据持久性，那么应该设置acks=all，这样当出现你说的这个情况时，producer会被显式通知消息发送失败，从而可以重试。
2019-06-17


5

Nero
好了，有了 240Mbps，我们就可以计算 1 小时内处理 1TB 数据所需的服务器数量了。根据这个目标，我们每秒需要处理 2336Mb 的数据， 老师这一段中2334Mb是怎么算出来的，没看懂，能否解释一下，谢谢。
作者回复: 1024*1024/3600*8
2019-06-15

2

5

墙角儿的花
弱弱的问一句老师，“根据这个目标，我们每秒需要处理 2336Mb 的数据，除以 240，约等于 10 台服务器”，机房入口带宽1Gbps,怎么能做到1秒处理2336Mb的数据的
作者回复: 这里是指单机带宽，机房总带宽不可能这么小的。。。
2019-06-15

1

4

WL
有三个问题请教一下老师:
1. 上文提到对于千兆网卡kafka服务器最多使用700M的带宽资源, 这700M的资源是单机使用的还是集群共用的, 为什么不能作为常规使用呢?
2. 文章举例是1小时1T的数据处理目标, 那一秒中是不是1024/3600 = 0.284G = 285M, 请问下文章中的2336M是咋算出来的.
3. 文章中的例子kafka单机要达到240M的读写能力, CPU应该配几核的?
作者回复: 1. 这个700Mb只是经验值罢了。另外预留buffer的意思是即使你最好不要让broker常规占用700Mb的资源。一旦碰到峰值流量，很容易将带宽打满。故做了一些资源预留
2. 285M是大B，即字节啊，乘以8之后就是2336Mb。带宽资源一般用Mbps而非MBps衡量
3. 我没有谈及CPU，是因为通常情况下Kafka不太占用CPU，因此没有这方面的最佳实践出来。但有些情况下Kafka broker是很耗CPU的：1. server和client使用了不同的压缩算法；2. server和client版本不一致造成消息格式转换；3. broker端解压缩校验

其中前两个都能规避，第三个目前无法规避。不过相比带宽资源，CPU通常都不是瓶颈
2019-06-15


3

Royal
您好，我想请教下kafka metric相关的知识，比如kafka produce速率获取等
作者回复: kafka producer速率可以监控这个JMX指标：

kafka.producer:type=[consumer|producer|connect]-node-metrics,client-id=([-.\w]+),node-id=([0-9]+)
2019-06-15

1

3

黄楚门的世界
老师您好,我们现在用的网络是10Gb,的万兆网.这样主要性能瓶性可会在硬盘IO上,我们配置了24坏的sas 机械盘。
刚才老师主要从容量跟网络性能,评估集群的规模。
我想问一下,怎么从硬盘的IO性能,去评估集群的规模?
还有做6个盘组成一个raid5,比直接用裸盘,性能会有多大的损失?
作者回复: RAID-5具体有多少性能损失不好说，但肯定会有的，最好还是以测试结果为准。另外咱们是否可以更多地利用Kafka在软件层面提供的高可靠性来保证数据完整性呢？不用单纯依赖于硬件。

至于评估方法，磁盘要相对复杂一些。毕竟当topic数量很多的时候磁盘不一定都是顺序写。不过你姑且可以做这样的测试：做一个单partition的topic，测试一下该topic对应的TPS，然后用你磁盘的TPS去核算单块磁盘上大概能放多少个partition。
2019-06-18

1

2

疯琴
老师，partitons的数量和硬盘的数量有匹配关系么？一块盘一个partiton比一块盘多个partiton要快么？是线性的关系么？
作者回复: 没有具体的关系。

“一块盘一个partiton比一块盘多个partiton要快么？” 没有实验数据支撑，单纯从分析角度来看我是认同的。当某块磁盘上有太多的分区时引入了很多随机IO拖慢了性能。事实上，阿里的RocketMQ一直宣称当单磁盘超过256分区时Kafka性能不如RocketMQ，原因也在于此。
数据来源：http://jm.taobao.org/2016/04/07/kafka-vs-rocketmq-topic-amout/
2019-06-17


2

Eagles
老师你好，Leader Follower 之间的数据冗余复制会不会占带宽？如果占带宽且有两个以上的follows，岂不是把预留的2/3的带宽全部用掉了。
作者回复: 会占带宽的，这也是预留的部分之一，就是给非业务活动留的带宽资源，包括操作系统其他进程消耗的带宽啊。
2019-06-17


2

小头针
在生产环境，实时推送过来的数据一份是直接存储到oracle，一份是接收后写入到kafka 对应的topic，storm程序消费后写入到kafka和elasticsearch ，经常会出现当天的oracle存储数据和elasticsearch存储的数据量有差异，有时候能差异率达到15%，胡老师这里面是否有带宽的原因呢？
作者回复: 不一定是带宽这么底层的问题，更像是Storm流处理的不正确导致的。Storm+Kafka要实现端到端的精确一次处理语义可谓是非常的难。事实上， 其他流处理框架也不敢保证百分之百的正确性。
2019-06-16

1

2

QQ怪
老师尽然把我模糊不清的GB和Gb的区别给复习了一遍😂
2019-06-15

1

2

夜影如歌
1亿条消息保存两份，是指一个leader一个副本吧
作者回复: 嗯嗯，是的。其实leader也是副本，副本有两类：leader和follower
2019-10-18


1

Geek_b809ff
老师，请问一直报这个错是因为什么：
 Error while fetching metadata with correlation id 101 : {test=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)

另外，server.properties里面下面这几个参数应该怎么配置：
listeners=PLAINTEXT://:9092
advertised.listeners=PLAINTEXT://:9092
istener.security.protocol.map

老师看到帮忙回复下，困扰一天了
作者回复: 你的主题test创建出了吗
2019-08-21

1

1

李跃爱学习
老师希望解答一下，之前也说明了Kafka 机器上没有混布其他服务，为什么常规需要预留2/3，只能跑240Mbps，
作者回复: 为follower拉取留一些带宽
2019-07-16


1

Bin滨
“根据这个目标，我们每秒需要处理 2336Mb 的数据，除以 240，约等于 10 台服务器， 如果消息还需要额外复制两份，那么总的服务器台数还要乘以 3，即 30 台。

请问一下， 这里说的30台服务器是指 10 个kafka cluster， 每个cluster 有3 brokers? 消息写道leader broker 的topic partition 上， 然后用参数 replication.factor 去做topic的消息备份吗? 多谢！


作者回复: 是指30台broker的意思：）
2019-06-19


1

疯琴
老师，如果单机起多个broker是否有可能造成同一个partition的多个副本在一台机器上，损失了容灾能力？
作者回复: 是的，有这个可能。现在的分布式集群都倾向于使用普通性能的机器搭建，因此单台单Broker性价比很高的。

当然我也知道很多公司采购了超强的服务器，然后在上面跑多个实例。这么做的原因是因为机器价格其实很便宜，贵的是IDC机架位（至少在北京是这样），因此两个方案都有自己合理的地方吧。
2019-06-17


1

AF
感觉有几个知识盲点：I/O模型、零拷贝和RAID。
2019-06-16


1

曹飞
老师您好，您文章中讲到磁盘方面可以抛弃RAID,直接使用普通磁盘上去。那么一旦这块磁盘出问题了，数据不就丢失了吗。如果raid的话，还有其他磁盘有相应的冗余数据的。麻烦帮忙解答下，谢谢！！！
作者回复: Kafka 1.1开始正式支持JBOD了。再说Kafka本身在软件层面也提供了冗余的机制来对抗磁盘损坏。
2019-06-15


1

我自成魔
您好！我在使用Kafka中，出现消息阻塞不消费的问题，换了消费组之后过段时间又不消费了，不知什么原因，望老师解惑。谢谢！
作者回复: 两个可能的原因，①就是没有新消息可供消费了，②某天消息格式有问题导致解析不了了，不过这种情况很罕见，一般是因为网络传输出问题导致
2019-06-15


1

Fouy_飞虎
您好，对于“带宽是 1Gbps，即每秒处理 1Gb 的数据”。1Gbps传输的是byte，我们通常说的存储是字节为单位的。8byte=1字节，所以我认为应该是“带宽是 1Gbps，即每秒处理 1/8 Gb 的数据”
作者回复: 带宽单位一般是小B，也就是比特。另外字节就是BYTE啊
2019-06-15

2

1

HenryZ
老师，对带宽计算有个疑问：如果副本树是1， 也就没有了follower 和 leader 的同步，那么还应该预留 2/3吗， 也就是 700Mb -> 240Mb 这一步？
作者回复: 那就不需要了
2019-11-13



Kong
最后乘的那个8是什么呀，
作者回复: Byte变bit：）
2019-10-15

1


song218888
读到本节为止，课程讲的非常好，特别是这一节关于部署集群需要考虑有哪些点，很受益
2019-10-13



云师兄
机器数量因为2个副本直接10×3，那原来10台中也可以互相存储副本，也有预留的2/3的带宽供使用，为什么是直接×3，望老师解答一下
作者回复: 抱歉，没太听懂。。。
2019-09-12



五年
这章超级赞 站在架构的角度进行设计
2019-09-07



megachilles
故通常要再额外预留出 2/3 的资源
——————为什么要额外预留出呢？
作者回复: 这只是保守估计，因为不想让kafka永远在这么高位吃这么多网卡
2019-09-05



wvalianty
老师，有一点似乎应该在这一节提一下，kafka的并发能力。
2019-08-30



Geek_b809ff
老师，请问一直报这个错是因为什么：
 Error while fetching metadata with correlation id 101 : {test=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)

另外，server.properties里面下面这几个参数应该怎么配置：
listeners=PLAINTEXT://:9092
advertised.listeners=PLAINTEXT://:9092
istener.security.protocol.map

test主题有创建，用其它的主题去尝试也是有问题。而且这个不是偶尔发生，每次生产消息的时候都有这个问题。这个跟zookeeper是否有关，看这个错误是LEADER_NOT_AVAILABLE
作者回复: 通常情况下你不用配置listener.security.protocol.map。其他两个配置感觉也没什么问题。你能否用console producer试下能否发送消息？
2019-08-22



doubleWei
请问有没有查看network和io请求处理时间的指标，通过这个来判断是服务处理网络请求慢了还是io请求满了，从而查看瓶颈点
作者回复: 通过kafka.network:type=RequestMetrics,name=RequestQueueTimeMs,request={Produce|FetchConsumer|FetchFollower} 判断是否需要调整num.network.threads
2019-08-22

1


godtrue
线上集群部署方案考量
1：软件
操作系统——觉得怎么和硬件打交道
2：硬件
硬盘——决定能存储多少数据，性能如何、带宽——决定网络传输的效率，以及是否会成为瓶颈

虽然都是顺序写，但是固态硬盘的性能还是比机械硬盘高的多，如果出于写磁盘性能的考虑，还是应该选固态硬盘。

至于jre、内存、CPU这些，我认为也是必须要考虑的，选择的依据又是什么呢？
作者回复: Java版本尽量选8，其他的没有太多参考依据。我觉得不需要到考虑内存条这一级吧。
2019-08-12



shjdwxy
我们线上遇到过这样的问题，多块磁盘之间负载不均衡，一块盘的io成了瓶颈。这样的话，是不是raid0是一个比较好的方案
作者回复: 感觉不是。实际环境中用RAID0的机会其实非常少，毕竟一块坏了就废了
2019-08-11

2


Giant
磁盘不做raid如果后面某台broker挂了，重新上线rebalance过程会消耗很多带宽，虽然可以减少topic保留时间来减少rebalance同步的数据量，但个人感觉还是做raid10比较靠谱，故障率低一些。不知道是否有关于这部分更好的解决方案？
作者回复: 嗯，没有绝对好的方案，根据实际需要选择合适的就行：）
2019-08-01



shern
一般数据流量都会先经过机房内的交换机然后再流到服务器。想请教下，实际规划过程中是不是还要考虑机房内的交换机与服务器的拓扑情况，这方面老师有经验或者案例分享吗？
2019-07-22



开水
回过头来看，忽然想起一个问题。老师，一个kafka topic定partition数量的时候最好以什么为参考依据呢？
2019-07-17

1


文洲
老师有个点有点疑问，“SLA 是在 1 小时内处理 1TB 的业务数据”，这个估算是按照入流量，还是出流量，按照kafka的模式，比如3个消费组同时消费的话，出流量还需*3
2019-07-13



金hb.Ryan 冷空氣駕到
请问对线上内存会有要求么
作者回复: 尽量大一点，最好10GB+以上的
2019-07-08



外星人
老师你好，麻烦问下，kafka的磁盘挂裸盘的话，如果出现磁盘坏的情况，该如何优雅的处理啊？
作者回复: 1.1之前不好处理，只能下掉坏磁盘之后重新配置log.dirs再重启broker；1.1之后至少能保证部分磁盘坏掉后broker不宕机，你可以直接下掉坏磁盘。
2019-07-05



小猪
老师，kafka有在k8s上部署的方案吗？
2019-07-04



icyricky
想问下在万兆网10Gbps的情况下，单台Kafka处理数据会成为瓶颈吗，像文中的举例，此时会是只需要3台吗？
作者回复: 这个要依据具体的使用情况来定了。
2019-06-25



itzzy
创建一个topic，partition的副本数设置为多少合适，从哪些方面考虑呢？老师会在后面讲到吗？
作者回复: 通过实测TPS和你的SLA共同决定
2019-06-23



silverhawk
windows上为什么不用IOCP做呢？这才是真的async nonblocking
作者回复: Java NIO并没有使用IOCP，Java NIO 2使用了IOCP。Kafka源码中慢慢在切换成nio 2，比如对文件的操作
2019-06-22



Bitson
“不能让 Kafka 服务器常规性使用这么多资源，故通常要再额外预留出 2/3 的资源”，请问预留这三分之二的带宽是出于什么考虑呢？
作者回复: 一般是为副本同步之用
2019-06-22



祥哥
你好，老师，弱弱问一下，每秒处理2336M数据，这个值如何计算出来的呢？谢谢
作者回复: 从大B换算成了小B
2019-06-20



黄楚门的世界
补充一下我的上个问题,如果机器单独做kafka ,一台机器broker 实例应该部署几个?根据什么标准去做估算?
作者回复: 本文介绍的是通用的评估方法，不限定一台机器上到底部署几台broker
2019-06-18



风中花
继续打卡 ，乐观学习，不懂装懂，继续前进，， 我这个windows 圈的老程，感觉压力颇大啊
作者回复: 加油：）
2019-06-18



无菇朋友
因为我基本上属于0基础 不知道看您这个教程会困难吗
作者回复: 专栏前面有几篇讲基础的，可以读一读试试。如果觉得可以入门再继续往下看：）
2019-06-18



PK時頭髮不亂
现在都是云服务，用的都是虚拟系统或虚拟系统的容器（docker），这种场景的部署方案跟这篇文章的物理机相差甚远，那要怎样衡量或要注意事项呢？
作者回复: 要衡量的因素或方法是相通的。另外云服务商（特别是大厂）都有自己的Kafka PaaS服务。如果全部都上公有云，也可以考虑直接用这些大厂的服务：）
2019-06-17



无菇朋友
老师 关于分区，副本这些概念，后面会再详细讲解吗？
作者回复: 后面还会穿插讲到一些。不过您有什么疑问吗？我可以在这里再详细说说：0
2019-06-17



莫问流年
分析带宽的时候，需要考虑副本机制带来的带宽损耗吗？或者说leader和follower之间采用不同网卡和宽带吗？
作者回复: 预留的部分已经考虑了副本机制引入的带宽了。
2019-06-17



Juc
老师请解释下：磁盘容量实际使用中建议预留20%-30%的磁盘空间包不包括为其他类型的数据（比如索引数据等）预留出的10%的磁盘空间呢？
作者回复: 预留的空间主要就是为Kafka索引文件预留的
2019-06-17



电光火石
在线上，通过kafka metric读到一个丢包率，之前一直没有想明白这个丢包是怎么出来的，broker也没有down过，是否就是由于网络带宽被打满造成的？谢谢了！
作者回复: Kafka metrics都是应用级别的监控指标，没有这种IT级别的指标（如果咱们理解的丢包率是相同的概念的话: ）

是否可以提供完整的Kafka Metrics名字我帮查查？
2019-06-16



Mango
老师，消息要额外复制两份，那么机器数为何要乘以三？是因为三台机器之间要耗费贷款，所以可处理实际数据的带宽相当于缩小到三分之一吗？
作者回复: 我想表达的是：如果副本数是1，消息只需要1份；如果还有两个follower，消息共保存3份，因此机器数变为了原来的3倍——这个假设是：follower与leader处于不同的broker而实际环境中不推荐单机多broker的架构
2019-06-16



lh
如果消息还需要额外复制两份，那么总的服务器台数还要乘以 3。
请问这里额外复制两份的消息指的是什么消息，两个followers上从lead上同步的消息吗？为什么复制的消息需要重复计算？
作者回复: 是的。因为follower必然与leader在不同的broker上，而实际生产环境中不推荐单机上部署多个broker——尽管你的服务器可能很强劲，起多个broker似乎没有问题，但对Kafka而言，带宽往往最先成为瓶颈，这个是单机规避不掉的。
2019-06-16



zhisheng
老师，如果要跨集群不停机迁移 Kafka 有什么可供参考的方案吗？

作者回复: 目前免费的方案就是Kafka自带的MirrorMaker了，不过据业界反映极不好用，主要是错误处理方面不太健壮。很多公司都或多或少地改写了MirrorMaker，比如Uber。
2019-06-15



Royal
您好，我的意思是Kafka produce等metric是怎么计算的？老师可以解答下么
作者回复: 这要取决于哪个JMX指标了，比如计算请求延时的JMX指标就是在producer端接收到响应之后计算的
2019-06-15



刚子
kafka 的分区数量的设置需要参考每秒传输的字节数计算吗？谢谢老师
作者回复: 通常不必这么细粒度。网上有一些分区制定的建议，我觉得这个粗粒度的方法就很好，值得一试：
1. 首先你要确定你业务的SLA，比如说你希望你的producer TPS是10万条消息/秒，假设是T1
2. 在你的真实环境中创建一个单分区的topic测试一下TPS，假设是T2
3. 你需要的分区数大致可以等于T1 / T2
3.
2019-06-15



Nero
明白了，这个是小转大要乘以8，谢谢老师
2019-06-15



兆祥
带宽是bit，规划的时候要除于八
2019-06-15



莱茵金属
能讲讲kafka的性能测试脚本怎么使用吗？
作者回复: 直接敲命令有参数提示，可以详细看下。另外我的建议是：kafka-producer-perf-test脚本还不错，kafka-consumer-perf-test有点难用
2019-06-15



13501018051
zookeeper部署方案和kafka一样么？分开部署还是部署到一起？我们现在都是一个机器上部署一个zookeeper和一个kafka
作者回复: 如果配置好ZooKeeper事务日志（比如设置好autopurge.purgeInterval及定期删除snapshot等），它对IO的需求不大，混布也是可以的。
2019-06-15






